{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "from scipy import stats\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. CSV to ARFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since we are using Python, we do  not need to complete this step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pandas.read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../../data/raw/\"\n",
    "\n",
    "X = pd.read_csv(f\"{file_path}x_train_gr_smpl.csv\", delimiter=',')\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = pd.read_csv(f\"{file_path}y_train_smpl.csv\", delimiter=',')\n",
    "Y.columns = ['target']\n",
    "\n",
    "y0 = pd.read_csv(f\"{file_path}y_train_smpl_0.csv\", delimiter=',')\n",
    "y0.columns = ['target']\n",
    "\n",
    "y1 = pd.read_csv(f\"{file_path}y_train_smpl_1.csv\", delimiter=',')\n",
    "y1.columns = ['target']\n",
    "\n",
    "y2 = pd.read_csv(f\"{file_path}y_train_smpl_2.csv\", delimiter=',')\n",
    "y2.columns = ['target']\n",
    "\n",
    "y3 = pd.read_csv(f\"{file_path}y_train_smpl_3.csv\", delimiter=',')\n",
    "y3.columns = ['target']\n",
    "\n",
    "y4 = pd.read_csv(f\"{file_path}y_train_smpl_4.csv\", delimiter=',')\n",
    "y4.columns = ['target']\n",
    "\n",
    "y5 = pd.read_csv(f\"{file_path}y_train_smpl_5.csv\", delimiter=',')\n",
    "y5.columns = ['target']\n",
    "\n",
    "y6 = pd.read_csv(f\"{file_path}y_train_smpl_6.csv\", delimiter=',')\n",
    "y6.columns = ['target']\n",
    "\n",
    "y7 = pd.read_csv(f\"{file_path}y_train_smpl_7.csv\", delimiter=',')\n",
    "y7.columns = ['target']\n",
    "\n",
    "y8 = pd.read_csv(f\"{file_path}y_train_smpl_8.csv\", delimiter=',')\n",
    "y8.columns = ['target']\n",
    "\n",
    "y9 = pd.read_csv(f\"{file_path}y_train_smpl_9.csv\", delimiter=',')\n",
    "y9.columns = ['target']\n",
    "\n",
    "Y.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_smpl_0 = pd.concat([X, y0], axis=1)\n",
    "train_smpl_1 = pd.concat([X, y1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_smpl_2 = pd.concat([X, y2], axis=1)\n",
    "train_smpl_3 = pd.concat([X, y3], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_smpl_4 = pd.concat([X, y4], axis=1)\n",
    "train_smpl_5 = pd.concat([X, y5], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_smpl_6 = pd.concat([X, y6], axis=1)\n",
    "train_smpl_7 = pd.concat([X, y7], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_smpl_8 = pd.concat([X, y8], axis=1)\n",
    "train_smpl_9 = pd.concat([X, y9], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_smpl = pd.concat([X, Y], axis=1)\n",
    "train_smpl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_smpl_0.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "train_smpl.hist(column='target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_smpl.target.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### imbalanced classes => won't generalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Randomisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklear.utils.suffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_smpl = shuffle(train_smpl, random_state=42)\n",
    "train_smpl_0 = shuffle(train_smpl_0, random_state=42)\n",
    "train_smpl_1 = shuffle(train_smpl_1, random_state=42)\n",
    "train_smpl_2 = shuffle(train_smpl_2, random_state=42)\n",
    "train_smpl_3 = shuffle(train_smpl_3, random_state=42)\n",
    "train_smpl_4 = shuffle(train_smpl_4, random_state=42)\n",
    "train_smpl_5 = shuffle(train_smpl_5, random_state=42)\n",
    "train_smpl_6 = shuffle(train_smpl_6, random_state=42)\n",
    "train_smpl_7 = shuffle(train_smpl_7, random_state=42)\n",
    "train_smpl_8 = shuffle(train_smpl_8, random_state=42)\n",
    "train_smpl_9 = shuffle(train_smpl_9, random_state=42)\n",
    "train_smpl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_smpl.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_smpl.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Reducing the size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Because the data runs the data as a Python file, we do not need to reduce the size of our data set. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - 6: NB, Features/Attributes Selection, & Improving #5's Classification  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "#plt.matshow(train_smpl.corr())\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Train Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_smpl[train_smpl.columns[:2303]], train_smpl['target'], test_size=0.33, random_state=42)\n",
    "\n",
    "X_train_20, X_test_20, y_train_20, y_test_20 = train_test_split(train_smpl_20[train_smpl_20.columns[:20]], train_smpl_20['target'], test_size=0.33, random_state=42)\n",
    "X_train_50, X_test_50, y_train_50, y_test_50 = train_test_split(train_smpl_50[train_smpl_50.columns[:50]], train_smpl_50['target'], test_size=0.33, random_state=42)\n",
    "X_train_100, X_test_100, y_train_100, y_test_100 = train_test_split(train_smpl_100[train_smpl_100.columns[:100]], train_smpl_100['target'], test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayes model (multi-class classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before Features/Attributes Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In regards to problem 4: Because we are not using Weka, we did not need to apply any\n",
    "# filters to the data before running Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "conf_mat = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True):\n",
    "    \"\"\"\n",
    "        https://stackoverflow.com/a/50386871\n",
    "    \"\"\"\n",
    "\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(conf_mat, target_names=y_test.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking for devious class labels that take high numbers of misclassifications\n",
    "for i in range(conf_mat.shape[1]):\n",
    "    column = conf_mat.T[i]\n",
    "    misclassifications = column.sum() - column[i]\n",
    "    print(misclassifications)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Naive Bayes on Multilabel dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n",
    "print(f\"Multinomial NB: {clf.score(X_test, y_test)} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. For each of the 10 train_smpl_label files, record the first 10 fields, in order of the absolute correlation value for each street sign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileList = [train_smpl_0, train_smpl_1, train_smpl_2, train_smpl_3, train_smpl_4,\n",
    "            train_smpl_5, train_smpl_6, train_smpl_7, train_smpl_8, train_smpl_9]\n",
    "for f, file in enumerate(fileList):\n",
    "    corrArr = []\n",
    "    for j in file.columns[:-1]:\n",
    "        corrVal, pVal = stats.pearsonr(file.iloc[:, int(j)], file.iloc[:, -1])\n",
    "        # Record the absolute value of the correlation\n",
    "        corrArr.append(math.fabs(corrVal))\n",
    "    print(f\"File {f}:\")\n",
    "    print(sorted([(x, i) for (i, x) in enumerate(corrArr)], reverse=True)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Naive Bayes using select features (20, 50, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods to run NB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_bernoulli_nb(data, num_feat, class_num):\n",
    "    X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(data[data.columns[:2303]], data['target'],\n",
    "                                                                test_size=0.33, random_state=42)\n",
    "\n",
    "    clf = BernoulliNB()\n",
    "    clf.fit(X_train_2, y_train_2)\n",
    "    print(f\"Bernoulli NB Best {num_feat}0 Class {class_num}: {clf.score(X_test_2, y_test_2)}\")\n",
    "\n",
    "def run_multi_nb_binary(data, num_feat, class_num):\n",
    "    X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(data[data.columns[:2303]], data['target'],\n",
    "                                                                test_size=0.33, random_state=42)\n",
    "    clf = MultinomialNB()\n",
    "    clf.fit(X_train_2, y_train_2)\n",
    "    print(f\"Multinomial for Binary NB Best {num_feat}0: Class {class_num}: {clf.score(X_test_2, y_test_2)}\")\n",
    "\n",
    "def run_multi_nb(data, num_feat):\n",
    "    X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(data[data.columns[:2303]], data['target'],\n",
    "                                                                test_size=0.33, random_state=42)\n",
    "    clf = MultinomialNB()\n",
    "    clf.fit(X_train_2, y_train_2)\n",
    "    print(f\"Multinomial NB Best {num_feat}0: {clf.score(X_test_2, y_test_2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Select top 2, 5, and 10 features for all train_smpl_label files to create best 20, 50, and 100 sets for all 10 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestSet = [2, 5, 10]\n",
    "\n",
    "for i, num in enumerate(bestSet):\n",
    "    k_best_list = pd.DataFrame()\n",
    "    # Get top k for each 10 classes\n",
    "    for f, file in enumerate(fileList):\n",
    "        k_best = SelectKBest(chi2, k=num).fit_transform(file[file.columns[:2303]], file['target'])\n",
    "        k_best = pd.DataFrame(k_best)\n",
    "        k_best_list = pd.concat([k_best_list, k_best], axis=1)\n",
    "        # print(k_best_list.head())\n",
    "\n",
    "    print(f\"\\nBinary NB Classifier {num}0:\\n __________________________________\")\n",
    "    # run binary NB classifiers for each class\n",
    "    for f, file in enumerate(fileList):\n",
    "        k_best_binary = pd.concat([k_best_list, file['target'].reset_index()], axis=1)\n",
    "        # print(f\"K BEST BINARY: {k_best_binary}\")\n",
    "        run_multi_nb_binary(k_best_binary, num, f)\n",
    "\n",
    "    print(f\"\\nMultilabel NB Classifier {num}0:\\n __________________________________\")\n",
    "    k_best_multi = pd.concat([k_best_list, train_smpl['target'].reset_index()], axis=1)\n",
    "    # print(f\"K BEST MULTI: {k_best_multi}\")\n",
    "    run_multi_nb(k_best_multi, num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis for 4 & 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Explain the reason for choosing and using these filters. Once you can run the algorithm, record, compare and analyse the classifier’s accuracy on different classes (as given by the Weka Summary and the confusion matrix)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since we are not using Weka, we did not need to apply any filters to the data before running Naive Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. What kind of information about this data set did you learn, as a result of the above experiments? You should ask questions such as: Which streets signs are harder to recognise? Which street signs are most easily confused? Which attributes (fields) are more reliable and which are less reliable in classification of street signs? What was the purpose of Tasks 5 and 6? What would happen if the data sets you used in Tasks 4, 5 and 6 were not randomised? What would happen if there is cross-correlation between the non-class attributes? You will get more marks for more interesting and ``out of the box” questions and answers. Explain your conclusions logically and formally, using the material from the lecture notes and from your own reading to interpret the results that Weka produces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions from initial experiments (q7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the multilabel confusion matrix, it is apparent that the multinomial Naïve Bayes classifier struggled with some classes, predicting them incorrectly most of the time. \n",
    "\n",
    "In particular, class 8 was more often classified as class 1 (probability 0.278), class 7 (probability 0.2023) and class 6 (probability 0.1974) before itself (probability 0.1283).\n",
    "\n",
    "//It looks like the classifier was confused by ...\n",
    "\n",
    "\n",
    "\n",
    "The strongest misclassification in the matrix is true members of class 3 being classified as class 7 (probability 0.2023). However, there is relatively very little of the reverse misclassification (true class 7 predicted as class 3).\n",
    "\n",
    "//^^ investigate this?\n",
    "\n",
    "\n",
    "\n",
    "Class 7 punches above its weight as the most abundant misclassification which is perhaps due to the relative infrequency of data for class 7. There must be some other factor, as there are less populated classes.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
