{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "from scipy import stats\n",
    "import math\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. CSV to ARFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#Since we are using Python, we do  not need to complete this step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pandas.read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "file_path = \"../../data/raw/\"\n",
    "\n",
    "X = pd.read_csv(f\"{file_path}x_train_gr_smpl.csv\", delimiter=',')\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# image = X.loc[[600]].values\n",
    "# image = image[0].reshape((48,48))\n",
    "# image.shape\n",
    "# plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "Y = pd.read_csv(f\"{file_path}y_train_smpl.csv\", delimiter=',')\n",
    "Y.columns = ['target']\n",
    "\n",
    "y0 = pd.read_csv(f\"{file_path}y_train_smpl_0.csv\", delimiter=',')\n",
    "y0.columns = ['target']\n",
    "\n",
    "y1 = pd.read_csv(f\"{file_path}y_train_smpl_1.csv\", delimiter=',')\n",
    "y1.columns = ['target']\n",
    "\n",
    "y2 = pd.read_csv(f\"{file_path}y_train_smpl_2.csv\", delimiter=',')\n",
    "y2.columns = ['target']\n",
    "\n",
    "y3 = pd.read_csv(f\"{file_path}y_train_smpl_3.csv\", delimiter=',')\n",
    "y3.columns = ['target']\n",
    "\n",
    "y4 = pd.read_csv(f\"{file_path}y_train_smpl_4.csv\", delimiter=',')\n",
    "y4.columns = ['target']\n",
    "\n",
    "y5 = pd.read_csv(f\"{file_path}y_train_smpl_5.csv\", delimiter=',')\n",
    "y5.columns = ['target']\n",
    "\n",
    "y6 = pd.read_csv(f\"{file_path}y_train_smpl_6.csv\", delimiter=',')\n",
    "y6.columns = ['target']\n",
    "\n",
    "y7 = pd.read_csv(f\"{file_path}y_train_smpl_7.csv\", delimiter=',')\n",
    "y7.columns = ['target']\n",
    "\n",
    "y8 = pd.read_csv(f\"{file_path}y_train_smpl_8.csv\", delimiter=',')\n",
    "y8.columns = ['target']\n",
    "\n",
    "y9 = pd.read_csv(f\"{file_path}y_train_smpl_9.csv\", delimiter=',')\n",
    "y9.columns = ['target']\n",
    "\n",
    "Y.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "train_smpl_0 = pd.concat([X, y0], axis=1)\n",
    "train_smpl_1 = pd.concat([X, y1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "train_smpl_2 = pd.concat([X, y2], axis=1)\n",
    "train_smpl_3 = pd.concat([X, y3], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "train_smpl_4 = pd.concat([X, y4], axis=1)\n",
    "train_smpl_5 = pd.concat([X, y5], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "train_smpl_6 = pd.concat([X, y6], axis=1)\n",
    "train_smpl_7 = pd.concat([X, y7], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "train_smpl_8 = pd.concat([X, y8], axis=1)\n",
    "train_smpl_9 = pd.concat([X, y9], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "train_smpl = pd.concat([X, Y], axis=1)\n",
    "train_smpl.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Randomisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn.utils.suffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "train_smpl = shuffle(train_smpl, random_state=42)\n",
    "train_smpl_0 = shuffle(train_smpl_0, random_state=42)\n",
    "train_smpl_1 = shuffle(train_smpl_1, random_state=42)\n",
    "train_smpl_2 = shuffle(train_smpl_2, random_state=42)\n",
    "train_smpl_3 = shuffle(train_smpl_3, random_state=42)\n",
    "train_smpl_4 = shuffle(train_smpl_4, random_state=42)\n",
    "train_smpl_5 = shuffle(train_smpl_5, random_state=42)\n",
    "train_smpl_6 = shuffle(train_smpl_6, random_state=42)\n",
    "train_smpl_7 = shuffle(train_smpl_7, random_state=42)\n",
    "train_smpl_8 = shuffle(train_smpl_8, random_state=42)\n",
    "train_smpl_9 = shuffle(train_smpl_9, random_state=42)\n",
    "train_smpl.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#train_smpl.hist(column='target')\n",
    "plt.hist(Y['target'], bins='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "train_smpl.target.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Reducing the size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Because the data runs the data as a Python file, we do not need to reduce the size of our data set. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "train_smpl.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler, NearMiss\n",
    "from imblearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# ros = RandomOverSampler(random_state=0)\n",
    "# X_oversampled, y_oversampled = ros.fit_resample(X, Y['target'])\n",
    "# plt.hist(y_oversampled, bins='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "oversample_ratio={2: 1000,  6: 1000, 7: 1000, 9: 1000}\n",
    "undersample_ratio={0: 1000, 1: 1000,  3: 1000, 4: 1000, 5: 1000, 8: 1000}\n",
    "pipe = make_pipeline(SMOTE(sampling_strategy=oversample_ratio, n_jobs=7), NearMiss(sampling_strategy=undersample_ratio, n_jobs=7))\n",
    "\n",
    "X_resampled, y_resampled = pipe.fit_resample(X, Y['target'])\n",
    "plt.hist(y_resampled, bins='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# rus = RandomUnderSampler(random_state=0)\n",
    "# X_undersampled, y_undersampled = rus.fit_sample(X, Y)\n",
    "# plt.hist(y_undersampled, bins='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# X_oversampled = pd.DataFrame(data=X_oversampled)\n",
    "# y_oversampled = pd.DataFrame(data=y_oversampled)\n",
    "# y_oversampled.columns=(['target'])\n",
    "# train_oversampled = pd.concat([X_oversampled, y_oversampled], axis=1)\n",
    "# train_oversampled = shuffle(train_oversampled, random_state=42)\n",
    "# train_oversampled.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "X_resampled = pd.DataFrame(data=X_resampled)\n",
    "y_resampled = pd.DataFrame(data=y_resampled)\n",
    "y_resampled.columns=(['target'])\n",
    "train_resampled = pd.concat([X_resampled, y_resampled], axis=1)\n",
    "train_resampled = shuffle(train_resampled, random_state=42)\n",
    "train_resampled.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# X_undersampled = pd.DataFrame(data=X_undersampled)\n",
    "# y_undersampled = pd.DataFrame(data=y_undersampled)\n",
    "# y_undersampled.columns=(['target'])\n",
    "# train_undersampled = pd.concat([X_undersampled, y_undersampled], axis=1)\n",
    "# train_undersampled = shuffle(train_undersampled, random_state=42)\n",
    "# train_undersampled.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - 6: NB, Features/Attributes Selection, & Improving #5's Classification  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## correlation matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%matplotlib inline\n",
    "\n",
    "#plt.matshow(train_smpl.corr())\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "print(f\"train_smpl shape:{train_smpl.shape}\")\n",
    "\n",
    "best_20 = SelectKBest(chi2, k=20).fit_transform(train_resampled[train_resampled.columns[:2303]], train_resampled['target'])\n",
    "best_20 = pd.DataFrame(best_20)\n",
    "train_smpl_20 = pd.concat([best_20, train_resampled['target']], axis=1)\n",
    "print(f\"train_smpl_20 shape:{train_smpl_20.shape}\")\n",
    "\n",
    "best_50 = SelectKBest(chi2, k=50).fit_transform(train_resampled[train_resampled.columns[:2303]], train_resampled['target'])\n",
    "best_50 = pd.DataFrame(best_50)\n",
    "train_smpl_50 = pd.concat([best_50, train_resampled['target']], axis=1)\n",
    "print(f\"train_smpl_50 shape:{train_smpl_50.shape}\")\n",
    "\n",
    "best_100 = SelectKBest(chi2, k=100).fit_transform(train_resampled[train_resampled.columns[:2303]], train_resampled['target'])\n",
    "best_100 = pd.DataFrame(best_100)\n",
    "train_smpl_100 = pd.concat([best_100, train_resampled['target']], axis=1)\n",
    "print(f\"train_smpl_100 shape:{train_smpl_100.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Train Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_smpl[train_smpl.columns[:2303]], train_smpl['target'], test_size=0.33, random_state=42)\n",
    "# X_train_ovr, X_test_ovr, y_train_ovr, y_test_ovr = train_test_split(train_oversampled[train_oversampled.columns[:2303]], train_oversampled['target'], test_size=0.33, random_state=42)\n",
    "X_train_res, X_test_res, y_train_res, y_test_res = train_test_split(train_resampled[train_resampled.columns[:2303]], train_resampled['target'], test_size=0.33, random_state=42)\n",
    "# X_train_udr, X_test_udr, y_train_udr, y_test_udr = train_test_split(train_undersampled[train_undersampled.columns[:2303]], train_undersampled['target'], test_size=0.33, random_state=42)\n",
    "\n",
    "X_train_20, X_test_20, y_train_20, y_test_20 = train_test_split(train_smpl_20[train_smpl_20.columns[:20]], train_smpl_20['target'], test_size=0.33, random_state=42)\n",
    "X_train_50, X_test_50, y_train_50, y_test_50 = train_test_split(train_smpl_50[train_smpl_50.columns[:50]], train_smpl_50['target'], test_size=0.33, random_state=42)\n",
    "X_train_100, X_test_100, y_train_100, y_test_100 = train_test_split(train_smpl_100[train_smpl_100.columns[:100]], train_smpl_100['target'], test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Multinomial Naive Bayes model (multi-class classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before Features/Attributes Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# In regards to problem 4: Because we are not using Weka, we did not need to apply any\n",
    "# filters to the data before running Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# clf = MultinomialNB()\n",
    "# clf.fit(X_train_ovr, y_train_ovr)\n",
    "# clf.score(X_test_ovr, y_test_ovr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resampled data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(X_train_res, y_train_res)\n",
    "clf.score(X_test_res, y_test_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# clf = MultinomialNB()\n",
    "# clf.fit(X_train_udr, y_train_udr)\n",
    "# clf.score(X_test_udr, y_test_udr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#Original data set\n",
    "y_pred = clf.predict(X_test)\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "#Resampled data set\n",
    "y_pred_res = clf.predict(X_test_res)\n",
    "conf_mat_res = confusion_matrix(y_test_res, y_pred_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True):\n",
    "    \"\"\"\n",
    "        https://stackoverflow.com/a/50386871\n",
    "    \"\"\"\n",
    "\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#Original data set\n",
    "plot_confusion_matrix(conf_mat, target_names=y_test.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#Resampled data set\n",
    "plot_confusion_matrix(conf_mat_res, target_names=y_test_res.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# looking for devious class labels that take high numbers of misclassifications\n",
    "for i in range(conf_mat.shape[1]):\n",
    "    column = conf_mat.T[i]\n",
    "    misclassifications = column.sum() - column[i]\n",
    "    print(i,misclassifications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# looking for devious class labels that take high numbers of misclassifications\n",
    "for i in range(conf_mat_res.shape[1]):\n",
    "    column = conf_mat_res.T[i]\n",
    "    misclassifications = column.sum() - column[i]\n",
    "    print(i,misclassifications)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. For each of the 10 train_smpl_label files, record the first 10 fields, in order of the absolute correlation value for each street sign."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "fileList = [train_smpl_0, train_smpl_1, train_smpl_2, train_smpl_3, train_smpl_4,\n",
    "            train_smpl_5, train_smpl_6, train_smpl_7, train_smpl_8, train_smpl_9]\n",
    "for f, file in enumerate(fileList):\n",
    "    corrArr = []\n",
    "    for j in file.columns[:-1]:\n",
    "        corrVal, pVal = stats.pearsonr(file.iloc[:, int(j)], file.iloc[:, -1])\n",
    "        # Record the absolute value of the correlation\n",
    "        corrArr.append(math.fabs(corrVal))\n",
    "    print(f\"File {f}:\")\n",
    "    print(\"10 Largest Correlation Values\")\n",
    "    print(sorted([(x, i) for (i, x) in enumerate(corrArr)], reverse=True)[:10])\n",
    "    print(\"10 Smallest Correlation Values\")\n",
    "    print(sorted([(x, i) for (i, x) in enumerate(corrArr)], reverse=True)[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Naive Bayes using select features (20, 50, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods to run NB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def run_multi_nb_binary(data, num_feat, class_num):\n",
    "    X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(data[data.columns[:2303]], data['target'],\n",
    "                                                                test_size=0.33, random_state=42)\n",
    "    clf = MultinomialNB()\n",
    "    clf.fit(X_train_2, y_train_2)\n",
    "    print(f\"Multinomial for Binary NB Best {num_feat}0: Class {class_num}: {clf.score(X_test_2, y_test_2)}\")\n",
    "\n",
    "def run_multi_nb(data, num_feat):\n",
    "    X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(data[data.columns[:2303]], data['target'],\n",
    "                                                                test_size=0.33, random_state=42)\n",
    "    clf = MultinomialNB()\n",
    "    clf.fit(X_train_2, y_train_2)\n",
    "    print(f\"Multinomial NB Best {num_feat}0: {clf.score(X_test_2, y_test_2)}\")\n",
    "    \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "def run_gaus_nb(data, num_feat):\n",
    "    X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(data[data.columns[:2303]], data['target'],\n",
    "                                                                test_size=0.33, random_state=42)\n",
    "    clf = GaussianNB()\n",
    "    clf.fit(X_train_2, y_train_2)\n",
    "    print(f\"Multinomial NB Best {num_feat}0: {clf.score(X_test_2, y_test_2)}\")\n",
    "    \n",
    "def run_gaus_nb_binary(data, num_feat, class_num):\n",
    "    X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(data[data.columns[:2303]], data['target'],\n",
    "                                                                test_size=0.33, random_state=42)\n",
    "    clf = GaussianNB()\n",
    "    clf.fit(X_train_2, y_train_2)\n",
    "    print(f\"Multinomial for Binary NB Best {num_feat}0: Class {class_num}: {clf.score(X_test_2, y_test_2)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Select top 2, 5, and 10 features for all train_smpl_label files to create best 20, 50, and 100 sets for all 10 classes - using original datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bestSet = [2, 5, 10, 75]\n",
    "bestDataDict = {} # copy dataframes for later\n",
    "\n",
    "for i, num in enumerate(bestSet):\n",
    "    k_best_list = pd.DataFrame()\n",
    "    # Get top k for each 10 classes\n",
    "    for f, file in enumerate(fileList):\n",
    "        k_best = SelectKBest(chi2, k=num).fit_transform(file[file.columns[:2303]], file['target'])\n",
    "        k_best = pd.DataFrame(k_best)\n",
    "        k_best_list = pd.concat([k_best_list, k_best], axis=1)\n",
    "        # print(k_best_list.head())\n",
    "\n",
    "    print(f\"\\nBinary NB Classifier {num}0:\\n __________________________________\")\n",
    "    # run binary NB classifiers for each class\n",
    "    for f, file in enumerate(fileList):\n",
    "        k_best_binary = pd.concat([k_best_list, file['target'].reset_index()], axis=1)\n",
    "        print(f\"K BEST BINARY shape: {k_best_binary.shape}\")\n",
    "        run_multi_nb_binary(k_best_binary, num, f)\n",
    "\n",
    "    print(f\"\\nMultilabel NB Classifier {num}0:\\n __________________________________\")\n",
    "    k_best_multi = pd.concat([k_best_list, train_smpl['target'].reset_index()], axis=1)\n",
    "    bestDataDict[f'{num}0'] = k_best_multi.copy()\n",
    "    print(f\"K BEST MULTI shape: {k_best_multi.shape}\")\n",
    "    run_multi_nb(k_best_multi, num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #Bonus part: running tests 5&6 on the resampled smpl_label dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the resampled train_smpl_label datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_binary_targets = []\n",
    "for i in range(0,10):\n",
    "    binary_arr = []\n",
    "    for j in train_resampled['target']:\n",
    "        if j == i:\n",
    "            binary_arr.append(0)\n",
    "        else:\n",
    "            binary_arr.append(1)\n",
    "    bin_arr = pd.DataFrame(binary_arr)\n",
    "    bin_arr.columns = ['target']\n",
    "    res_binary_targets.append(bin_arr)\n",
    "\n",
    "#remove multilabel target column\n",
    "train_res = train_resampled.drop(labels='target', axis=1)\n",
    "# train_res.head()\n",
    "# train_resampled['target'].head()\n",
    "\n",
    "#add binary target for each class\n",
    "full_res_binary_targets = []\n",
    "for idx in range(0, 10):\n",
    "    binary_df = pd.concat([train_res, res_binary_targets[idx]], axis=1)\n",
    "    full_res_binary_targets.append(binary_df)\n",
    "#     print(binary_df)\n",
    "    \n",
    "# full_res_multi_targets = pd.concat([train_res, res_binary_targets[idx]], axis=1)\n",
    "    \n",
    "full_res_binary_targets[:3]\n",
    "# print(full_res_binary_targets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #5: Resampled data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f, file in enumerate(full_res_binary_targets):\n",
    "    corrArr = []\n",
    "    for j in file.columns[:-1]:\n",
    "        corrVal, pVal = stats.pearsonr(file.iloc[:, int(j)], file.iloc[:, -1])\n",
    "        # Record the absolute value of the correlation\n",
    "        corrArr.append(math.fabs(corrVal))\n",
    "    print(f\"File {f}:\")\n",
    "    print(\"10 Largest Correlation Values\")\n",
    "    print(sorted([(x, i) for (i, x) in enumerate(corrArr)], reverse=True)[:10])\n",
    "    print(\"10 Smallest Correlation Value\")\n",
    "    print(sorted([(x, i) for (i, x) in enumerate(corrArr)], reverse=True)[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #6: Resampled data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "bestSet = [1, 2, 5, 10]\n",
    "\n",
    "for i, num in enumerate(bestSet):\n",
    "    k_best_list = pd.DataFrame()\n",
    "    # Get top k for each 10 classes\n",
    "    for f, file in enumerate(full_res_binary_targets):\n",
    "        k_best = SelectKBest(chi2, k=num).fit_transform(file[file.columns[:2303]], file['target'])\n",
    "        k_best = pd.DataFrame(k_best)\n",
    "        k_best_list = pd.concat([k_best_list, k_best], axis=1)\n",
    "#     print(k_best_list)\n",
    "\n",
    "    print(f\"\\nBinary NB Classifier {num}0:\\n __________________________________\")\n",
    "    # run binary NB classifiers for each class\n",
    "    for f, file in enumerate(full_res_binary_targets):\n",
    "        k_best_binary = pd.concat([k_best_list, file['target'].reset_index()], axis=1)\n",
    "#         print(f\"K BEST BINARY: {kbb}\")\n",
    "        run_gaus_nb_binary(k_best_binary, num, f)\n",
    "#         run_multi_nb_binary(k_best_binary, num, f)\n",
    "\n",
    "    print(f\"\\nMultilabel NB Classifier {num}0:\\n __________________________________\")\n",
    "    k_best_multi = pd.concat([k_best_list, train_resampled['target'].reset_index()], axis=1)\n",
    "#     print(f\"K BEST MULTI: {kbm}\")\n",
    "    run_gaus_nb(k_best_multi, num)\n",
    "#     run_multi_nb(k_best_multi, num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis for 4 & 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Explain the reason for choosing and using these filters. Once you can run the algorithm, record, compare and analyse the classifier’s accuracy on different classes (as given by the Weka Summary and the confusion matrix)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since we are not using Weka, we did not need to apply any filters to the data before running Naive Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. What kind of information about this data set did you learn, as a result of the above experiments? You should ask questions such as: Which streets signs are harder to recognise? Which street signs are most easily confused? Which attributes (fields) are more reliable and which are less reliable in classification of street signs? What was the purpose of Tasks 5 and 6? What would happen if the data sets you used in Tasks 4, 5 and 6 were not randomised? What would happen if there is cross-correlation between the non-class attributes? You will get more marks for more interesting and ``out of the box” questions and answers. Explain your conclusions logically and formally, using the material from the lecture notes and from your own reading to interpret the results that Weka produces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions from initial experiments (q7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the multilabel confusion matrix, it is apparent that the multinomial Naïve Bayes classifier struggled with some classes, predicting them incorrectly most of the time. \n",
    "\n",
    "In particular, class 8 was more often classified as class 1 (probability 0.278), class 7 (probability 0.2023) and class 6 (probability 0.1974) before itself (probability 0.1283).\n",
    "\n",
    "//It looks like the classifier was confused by ...\n",
    "\n",
    "\n",
    "\n",
    "The strongest misclassification in the matrix is true members of class 3 being classified as class 7 (probability 0.2023). However, there is relatively very little of the reverse misclassification (true class 7 predicted as class 3).\n",
    "\n",
    "//^^ investigate this?\n",
    "\n",
    "\n",
    "\n",
    "Class 7 punches above its weight as the most abundant misclassification which is perhaps due to the relative infrequency of data for class 7. There must be some other factor, as there are less populated classes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complex Bayesian Networks\n",
    "\n",
    "scikit learn has naive bayes builtin, but not the more complex networks. Weka can be called on to fill the gap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K2 Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import weka.core.jvm as jvm\n",
    "jvm.start(max_heap_size='7g') # Hopefully big enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "data_train_20 = pd.concat([X_train_20, y_train_20], axis=1)\n",
    "data_test_20 = pd.concat([X_test_20, y_test_20], axis=1)\n",
    "data_20_features = pd.concat([X_train_20, X_test_20], axis=0) # ask weka to split later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weka.core.classes as wkcore\n",
    "from weka.classifiers import Classifier, Evaluation\n",
    "import os\n",
    "\n",
    "# cmdline doesn't seem to be working - use the basic classes directly (or at least using the options list)\n",
    "# cls = Classifier(classname=\"weka.classifiers.trees.J48\", options=[\"-C\", \"0.3\"])\n",
    "\n",
    "# Weka's naive Bayes, to give comparable results to the sklearn method\n",
    "NBClassifier = Classifier(classname=\"weka.classifiers.bayes.NaiveBayes\")\n",
    "\n",
    "# k2, 1-4 parents:  \n",
    "k2classifiers = []\n",
    "for i in range(1,5):\n",
    "    classifier = Classifier(classname=\"weka.classifiers.bayes.BayesNet\", options=[\"-D\", \"-Q\", \"weka.classifiers.bayes.net.search.local.K2\", \"--\", \"-P\", f\"-{i}\", \"-S\", \"BAYES\", \"-E\", \"weka.classifiers.bayes.net.estimate.SimpleEstimator\", \"--\", \"-A\", \"0.5\"])\n",
    "    k2classifiers.append(classifier)\n",
    "\n",
    "# TAN \n",
    "tanClassifier = Classifier(classname=\"weka.classifiers.bayes.BayesNet\", options=[\"-D\", \"-Q\", \"weka.classifiers.bayes.net.search.local.TAN\", \"--\", \"-S\", \"BAYES\", \"-E\", \"weka.classifiers.bayes.net.estimate.SimpleEstimator\", \"--\", \"-A\", \"0.5\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't downsize - use it all! train_resampled has some 12k instances, so this takes a while\n",
    "# use cached tsne results if we can, we don't have all day\n",
    "from sklearn.manifold import TSNE\n",
    "import pickle\n",
    "try:\n",
    "    with open('X_train_2D_t_emb.pickle', 'rb') as f:\n",
    "        unpickled = pickle.load(f)\n",
    "        X_train_2D_t_emb = unpickled\n",
    "except: # no file \n",
    "    print('No cache, running t-SNE')\n",
    "    X_train_2D_t_emb = TSNE(perplexity=30).fit_transform(X_train_res)\n",
    "    with open('X_train_2D_t_emb.pickle', 'wb') as f:\n",
    "        pickle.dump(X_train_2D_t_emb, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(x=unpickled[:,0], y=unpickled[:,1], c=y_train_res)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# fig = plt.figure()\n",
    "# ax = Axes3D(fig)\n",
    "# for label in range(10):\n",
    "#     x_train_tmp = X_wee_3_embeds[y_train_wee==label]\n",
    "#     ax.scatter(x_train_tmp[:,0], x_train_tmp[:,1], x_train_tmp[:,2], \n",
    "#                alpha=0.75, label=label)\n",
    "# ax.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# receive arff\n",
    "# from weka.core.converters import Loader\n",
    "# loader = Loader(classname=\"weka.core.converters.ArffLoader\")\n",
    "# arffData = loader.load_file(os.getcwd() + '/' + 'data_train_20.arff')\n",
    "\n",
    "# Use ndarray_to_instances to convert numpy -> weka\n",
    "import weka.core.converters as wekaconv\n",
    "y_train_res_np = y_train_res.to_numpy(copy=True).reshape(-1, 1)\n",
    "\n",
    "best100np_alt = bestDataDict['100'].to_numpy().copy(order='C')\n",
    "print(best100np_alt.shape)\n",
    "\n",
    "best100np = train_smpl_100.to_numpy().copy(order='C')\n",
    "\n",
    "weka_train = wekaconv.ndarray_to_instances(best100np_alt, \"train_resampled\")\n",
    "weka_train.class_is_last()\n",
    "\n",
    "print(weka_train.get_instance(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix \"class is numeric\"\n",
    "# First create a Filter to do the job\n",
    "from weka.filters import Filter\n",
    "nominaliser = Filter(classname='weka.filters.unsupervised.attribute.NumericToNominal', options=['-R', 'last']) \n",
    "renominaliser = Filter(classname='weka.filters.unsupervised.attribute.NumericToNominal', options=['-R', 'last']) \n",
    "\n",
    "randomiser = Filter(classname='weka.filters.unsupervised.instance.Randomize', options=['-S', '42'])\n",
    "\n",
    "# Standardise\n",
    "standardiser = Filter(classname='weka.filters.unsupervised.attribute.Standardize') \n",
    "\n",
    "# Use PCA to reduce dimensionality? Default Weka settings\n",
    "# This option was too expensive.\n",
    "# pcaFilter = Filter(classname='weka.filters.unsupervised.attribute.PrincipalComponents', options=['-R', '0.95', '-A', '5', '-M', '-1']) \n",
    "\n",
    "# Finally, apply the filters to the data\n",
    "nominaliser.inputformat(weka_train)\n",
    "standardiser.inputformat(weka_train)\n",
    "# pcaFilter.inputformat(weka_train)\n",
    "randomiser.inputformat(weka_train)\n",
    "\n",
    "# necessary for getting the input formats\n",
    "partFilteredData = randomiser.filter(standardiser.filter(nominaliser.filter(weka_train)))\n",
    "\n",
    "# \"class is numeric\" reappears after filter applications. We still need to do it at the start\n",
    "# so that the class isn't standardised in that filter\n",
    "renominaliser.inputformat(partFilteredData)\n",
    "\n",
    "filterSet = [nominaliser, standardiser, randomiser, renominaliser]\n",
    "\n",
    "filteredData = renominaliser.filter(partFilteredData)\n",
    "filteredData.class_is_last()\n",
    "\n",
    "print(filteredData.get_instance(5000))\n",
    "from weka.core.converters import Saver\n",
    "saver = Saver(classname=\"weka.core.converters.ArffSaver\")\n",
    "saver.save_file(weka_train, \"weka_train.arff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define experiment\n",
    "from weka.core.classes import Random\n",
    "from weka.classifiers import FilteredClassifier\n",
    "\n",
    "def curry_filters(arg_filterSet, arg_classifier):\n",
    "    '''\n",
    "    If you count a classifier as a function, then each loop inside this is kind of a partial application.\n",
    "    '''\n",
    "    fc = FilteredClassifier()\n",
    "\n",
    "    for i, filt in enumerate(arg_filterSet):\n",
    "        tmp_fc = FilteredClassifier()\n",
    "        tmp_fc.classifier = Classifier.make_copy(arg_classifier) if i == 0 else fc\n",
    "        tmp_fc.filter = filt\n",
    "        fc = tmp_fc\n",
    "    return fc\n",
    "\n",
    "def run_weka_test(data, classifier):\n",
    "    eva = Evaluation(data)\n",
    "    eva.evaluate_train_test_split(classifier, data, 66.7, rnd=Random(42))\n",
    "    return eva\n",
    "\n",
    "def print_results(evaluation, name):\n",
    "    print(f\"{evaluation.summary()}\")\n",
    "    plot_confusion_matrix(evaluation.confusion_matrix, target_names=y_train_res.unique().sort(), title=name)\n",
    "    return evaluation\n",
    "\n",
    "# def save_results(name, evaluation):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test with simple feature selection \n",
    "print(f\"\\nK2, 1 parent:\\n __________________________________\")\n",
    "print(data_train_20.shape)\n",
    "arff_data_train_20 = wekaconv.ndarray_to_instances(data_train_20.to_numpy().copy(order='C'), \"20_features\")\n",
    "arff_data_train_20.class_is_last()\n",
    "\n",
    "nominaliser_side = Filter(classname='weka.filters.unsupervised.attribute.NumericToNominal', options=['-R', 'last']) \n",
    "nominaliser_side.inputformat(arff_data_train_20)\n",
    "print(f\"data: {arff_data_train_20.num_attributes} attributes, {arff_data_train_20.num_instances} instances\")\n",
    "\n",
    "evalu = run_weka_test(nominaliser_side.filter(arff_data_train_20), curry_filters([nominaliser_side], k2classifiers[0]))\n",
    "print_results(evalu, 'K2 Classifier; 1 parent, 20 raw features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run experiment\n",
    "\n",
    "# Naive Bayes\n",
    "print(f\"\\nNaive Bayes Classifier (for comparison):\\n __________________________________\")\n",
    "print(f\"Data: {filteredData.num_attributes} attributes, {filteredData.num_instances} instances\")\n",
    "nominal_target_data = nominaliser.filter(weka_train)\n",
    "\n",
    "print_results(run_weka_test(nominal_target_data, curry_filters(filterSet, NBClassifier)), \"Naive Bayes Classifier; 100 features\")\n",
    "\n",
    "# K2\n",
    "for i, cls in enumerate(k2classifiers):\n",
    "\n",
    "    s = 's' if i != 0 else ''\n",
    "    title = f\"K2 Classifier, {i+1} Parent{s}; 100 features\"\n",
    "    print(f\"\\n{title}:\\n __________________________________\")\n",
    "    print_results(run_weka_test(nominal_target_data, curry_filters(filterSet, cls)), title)\n",
    "\n",
    "# TAN\n",
    "print(f\"\\nTAN Classifier:\\n __________________________________\")\n",
    "print_results(run_weka_test(nominal_target_data, curry_filters(filterSet, tanClassifier)), \"TAN Classifier; 100 features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save trained classifiers\n",
    "import weka.core.serialization as cereal\n",
    "cereal.write('evalu.saved', evalu)\n",
    "for i, cls in enumerate(k2classifiers):\n",
    "    s = 's' if i!=0 else ''\n",
    "    cereal.write(f'k2classifier{i+1}parent{s}.saved', k2classifiers[i])\n",
    "\n",
    "# with open('tanClassifier.pickle', 'wb') as f:\n",
    "#     # Pickle the 'data' dictionary using the highest protocol available.\n",
    "#     pickle.dump(tanClassifier, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_tan = Classifier(jobject=cereal.read('tanClassifier.saved'))\n",
    "print(\"read_tan:\\n\", read_tan)\n",
    "print(\"original tan:\\n\", tanClassifier)\n",
    "# read_evalu = Evaluation(jobject=cereal.read('evalu.saved'))\n",
    "\n",
    "# print(\"read evalu:\\n\", read_evalu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jvm.stop() # finished with weka; release the memory - we're going to need it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# scaler = MinMaxScaler(feature_range=[0, 1])\n",
    "\n",
    "# pca = PCA(n_components=50)\n",
    "# smpl_data = pd.DataFrame(pca.fit_transform(train_resampled[train_resampled.columns[:-1]]))\n",
    "# smpl_data = pd.DataFrame(scaler.fit_transform(smpl_data))\n",
    "# smpl_data[\"target\"] = train_smpl[\"target\"]\n",
    "\n",
    "# Training sample for train_smpl\n",
    "\n",
    "smpl_data = train_smpl.copy()\n",
    "\n",
    "kmeans = KMeans(n_clusters=10, random_state=2)\n",
    "smpl_data[\"cluster\"] = kmeans.fit_predict(smpl_data[smpl_data.columns[:-1]])\n",
    "# smpl_data[\"w_class_attribute\"] = kmeans.fit_predict(smpl_data[smpl_data.columns[:-1]])\n",
    "\n",
    "resampled_data = train_resampled.copy()\n",
    "\n",
    "kmeans = KMeans(n_clusters=10, random_state=2)\n",
    "resampled_data[\"cluster\"] = kmeans.fit_predict(resampled_data[resampled_data.columns[:-1]])\n",
    "# resampled_data[\"w_class_attribute\"] = kmeans.fit_predict(resampled_data[resampled_data.columns[:-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Training sample for train_smpl_<label> \n",
    "training_dataframe = [train_smpl_0.copy(), train_smpl_1.copy(), train_smpl_2.copy(), train_smpl_3.copy(), train_smpl_4.copy(),\n",
    "                      train_smpl_5.copy(), train_smpl_6.copy(), train_smpl_7.copy(), train_smpl_8.copy(), train_smpl_9.copy()]\n",
    "\n",
    "kmeans = KMeans(n_clusters=2, random_state=2)\n",
    "for dataframe in training_dataframe:\n",
    "    dataframe[\"cluster\"] = kmeans.fit_predict(dataframe[dataframe.columns[:-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# print(target.groupby(['target']).count())\n",
    "# result_class_attribute = result.groupby(['target', 'cluster_class_attribute']).count()\n",
    "# result_cluster[\"class\"] = result_class_attribute[\"cluster\"]\n",
    "\n",
    "def plot_clustering(result, nb_attribute, nb_cluster, name_cluster=\"cluster\"):\n",
    "    df = result[result.columns[-3:]]\n",
    "    result_cluster = df.groupby(['target', name_cluster]).count()\n",
    "    result_cluster.reset_index(inplace=True)\n",
    "    result_data = []\n",
    "    for i in range(0, nb_cluster):\n",
    "        result_data.append([])\n",
    "        if name_cluster == \"w_class_attribute\":\n",
    "            data = result_cluster[result_cluster.w_class_attribute == i].drop([name_cluster], axis=1).to_dict('split')['data']\n",
    "        else:\n",
    "            data = result_cluster[result_cluster.cluster == i].drop([name_cluster], axis=1).to_dict('split')['data']\n",
    "        data = [{x[0]: x[1]} for x in data]\n",
    "        data = dict(pair for d in data for pair in d.items())\n",
    "        for j in range(0, nb_attribute):\n",
    "            result_data[i].append(data[j] if j in data else 0)\n",
    "\n",
    "    data = np.array(result_data)\n",
    "\n",
    "    color_list = ['#FC7A57', '#EEFC57', '#5E5B52', '#8DB580', '#BBE5ED', '#4281A4', '#BAD9B5', '#CA7DF9', '#04E762', '#723D46']\n",
    "\n",
    "    X = np.arange(data.shape[1])\n",
    "    for i in range(data.shape[0]):\n",
    "        plt.bar(X, data[i],\n",
    "            bottom = np.sum(data[:i], axis = 0),\n",
    "            color = color_list[i % len(color_list)],\n",
    "               label=f\"Cluster {i}\")\n",
    "\n",
    "    plt.legend(loc=\"best\", bbox_to_anchor=(1.0, 1.00))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Plot train_smpl_<label> clusters\n",
    "i = 0\n",
    "for dataframe in training_dataframe:\n",
    "    print(f\"Training sample n°{i}:\")\n",
    "    plot_clustering(dataframe, 10, 2)\n",
    "    i += 1\n",
    "\"\"\"\n",
    "# Plot train_smpl clusters\n",
    "print(f\"Training sample all data (without class attribute):\")\n",
    "plot_clustering(smpl_data, 10, 10)\n",
    "# print(f\"Training sample all data (with class attribute):\")\n",
    "# plot_clustering(smpl_data, 10, 10, 'w_class_attribute')\n",
    "\n",
    "# Plot train_smpl resampled clusters\n",
    "print(f\"Training resample data (without class attribute):\")\n",
    "plot_clustering(resampled_data, 10, 10)\n",
    "# print(f\"Training resample data (with class attribute):\")\n",
    "# plot_clustering(resampled_data, 10, 10, 'w_class_attribute')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Clustering notes\n",
    "\n",
    "The clustering classification is not as good as the Gaussian Naives model. It struggle with attributing the classes to the good target.\n",
    "\n",
    "When it is trained with the individual sample the two cluster doesn't separate the specific class from the other classes. But for the sample n°2 all the class is in one cluster, but the cluster also contains a lot of other classes.\n",
    "\n",
    "The classification obtained with and without class attribute are the same. Maybe we should change weight on the class attribute so there is more impact on the distribution.\n",
    "\n",
    "I did some tests with using PCA (Principal component analysis) which convert a set of observations of possibly correlated variables to reduce the number of attributes of the samples. And some other with the MinMax scaler. But theses preprocess didn't improve the result, the cluster are still very distributed among the classes.\n",
    "\n",
    "When we are comparing the result between basic sample of all data and the resample one, in both of the results the cluster are very distrubuted amoug the different classes. But we can notice in the resample result, that for the target 6 and 7, 9 they have almost a dedicated a cluster (corresponding in the order of cluster 5, 8 and 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Other clustering algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=50)\n",
    "smpl_data_pca = pd.DataFrame(pca.fit_transform(train_resampled[train_resampled.columns[:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import OPTICS\n",
    "\n",
    "smpl_data_optics = smpl_data_pca.copy()\n",
    "\n",
    "optics = OPTICS(min_samples=45, max_eps=450)\n",
    "smpl_data_optics[\"target\"] = train_resampled[\"target\"]\n",
    "smpl_data_optics[\"cluster\"] = optics.fit_predict(smpl_data_optics[smpl_data_optics.columns[:-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import Birch\n",
    "\n",
    "smpl_data_birch = smpl_data_pca.copy()\n",
    "\n",
    "birch = Birch(n_clusters=10)\n",
    "smpl_data_birch[\"target\"] = train_resampled[\"target\"]\n",
    "smpl_data_birch[\"cluster\"] = birch.fit_predict(smpl_data_birch[smpl_data_birch.columns[:-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import MeanShift\n",
    "\n",
    "smpl_data_mean_shift = smpl_data_pca.copy()\n",
    "\n",
    "mean_shift = MeanShift(bandwidth=2300)\n",
    "smpl_data_mean_shift[\"target\"] = train_resampled[\"target\"]\n",
    "smpl_data_mean_shift[\"cluster\"] = mean_shift.fit_predict(smpl_data_mean_shift[smpl_data_mean_shift.columns[:-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Training resample data with OPTICS clustering:\")\n",
    "plot_clustering(smpl_data_optics, 10, max(optics.labels_))\n",
    "\n",
    "print(f\"Training resample data with Birch clustering:\")\n",
    "plot_clustering(smpl_data_birch, 10, 10)\n",
    "\n",
    "print(f\"Training resample data with MeanShift clustering:\")\n",
    "plot_clustering(smpl_data_mean_shift, 10, max(mean_shift.labels_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Other clustering algorithms notes\n",
    "\n",
    "We are testing the resampled dataset with 3 different clustering algorithm: Optics, Birch and MeanShift. To improving speed of theses clustering algorithms, we are using PCA which reduce the number of component to analyse.\n",
    "\n",
    "Optics algorithm views clusters as areas of high density separated by areas of low density. Clusters found by Optics can be any shape and Optics can also ignore sample so it won't classify to any cluster. We use parameter for Optics 45 sample minimum in each cluster and 450 for the density necessary to form a cluster.\n",
    "\n",
    "Birch algorithm reduces the input data to a set of subclusters and then do the clustering on theses subclusters. For this algorithm we are using the default value of scikit learn which are 0.5 threshold and 50 for branching factor.\n",
    "\n",
    "MeanShift algorithm is a centroid based algorithm, which works by updating candidates for centroids to be the mean of the points within a given region. We are using 2300 for bandwidth. \n",
    "\n",
    "Even we other clustering algorithms the cluster result are not very successful, it doesn't separate the classes and for each test there are about the same proportion of cluster for each class.\n",
    "\n",
    "\n",
    "(The descriptions of algorithms is coming from: https://scikit-learn.org/stable/modules/clustering.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Research Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create datasets that have 1 class removed - original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "#Remove 2, 9, and 6\n",
    "rem_images = [2, 9, 6]\n",
    "train_smpl_copy_2 = copy.deepcopy(train_smpl)\n",
    "train_smpl_copy_9 = copy.deepcopy(train_smpl)\n",
    "train_smpl_copy_6 = copy.deepcopy(train_smpl)\n",
    "dataSet = [train_smpl_copy_2, train_smpl_copy_9, train_smpl_copy_6]\n",
    "removed_classes = []\n",
    "for d, data in enumerate (dataSet):\n",
    "    #Iterate through rows and identify ones that have the key \n",
    "    print(f\"d: {d}\")\n",
    "    removed_class = []\n",
    "    for idx, row in data.iterrows():\n",
    "        if row['target'] == rem_images[d]:\n",
    "            removed_class.append(row)\n",
    "            data.drop([idx], inplace=True)\n",
    "    removed_class_df = pd.DataFrame(removed_class)\n",
    "    removed_classes.append(removed_class_df)\n",
    "    print(\"~~~~NEXT~~~~\")\n",
    "\n",
    "print(f\"removed classes: {removed_classes[0]}\")\n",
    "# train_smpl_copy_6.head()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train smpl copy 6: {train_smpl_copy_6}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(removed_classes[0].shape)\n",
    "print(removed_classes[1].shape)\n",
    "print(removed_classes[2].shape)\n",
    "\n",
    "print(f\"Rem classes: {removed_classes[0]}\")\n",
    "# print(f\"Rem classes: {removed_classes[1]}\")\n",
    "# print(f\"Rem classes: {removed_classes[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method for plotting frequency of classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from: https://stackoverflow.com/questions\n",
    "# /50845998/matplotlib-bar-chart-that-shows-frequency-of-unique-values\n",
    "\n",
    "def make_plot(title, result):\n",
    "    plt.hist(result, bins=np.arange(10)-0.5)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Classes\")\n",
    "    plt.ylabel(\"No of Classifications\")\n",
    "    plt.xticks(ticks=[0,1,2,3,4,5,6,7,8,9,10],label=[0,1,2,3,4,5,6,7,8,9,10])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Multinomial NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # new instances where we do not know the answer\n",
    "# Xnew, _ = make_blobs(n_samples=3, centers=2, n_features=2, random_state=1)\n",
    "# # make a prediction\n",
    "# ynew = model.predict(Xnew)\n",
    "# # show the inputs and predicted outputs\n",
    "# for i in range(len(Xnew)):\n",
    "# \tprint(\"X=%s, Predicted=%s\" % (Xnew[i], ynew[i]))\n",
    "\n",
    "idx = [0,1,2,3,4,5,6,7,8,9]\n",
    "#Image 2 removed\n",
    "clf = MultinomialNB()\n",
    "clf.fit(train_smpl_copy_2.iloc[:,:-1], train_smpl_copy_2.iloc[:,-1])\n",
    "result_2 = clf.predict(removed_classes[0].iloc[:,:-1])\n",
    "make_plot(\"Image 2 Removed - 80 limit lifted\", result_2)\n",
    "print(f\"Result 2: {result_2}\")\n",
    "\n",
    "\n",
    "#Image 9 removed\n",
    "clf = MultinomialNB()\n",
    "clf.fit(train_smpl_copy_9.iloc[:,:-1], train_smpl_copy_9.iloc[:,-1])\n",
    "result_9 = clf.predict(removed_classes[1].iloc[:,:-1])\n",
    "make_plot(\"Image 9 Removed - Arrow\", result_9)\n",
    "print(f\"Result 9: {result_9}\")\n",
    "\n",
    "#Image 6 removed\n",
    "clf = MultinomialNB()\n",
    "clf.fit(train_smpl_copy_6.iloc[:,:-1], train_smpl_copy_6.iloc[:,-1])\n",
    "result_6 = clf.predict(removed_classes[2].iloc[:,:-1])\n",
    "make_plot(\"Image 6 Removed - Stop Sign\", result_6)\n",
    "print(f\"Result 6: {result_6}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
