{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. CSV to ARFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since we are using Python, we do  not need to complete this step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pandas.read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../../data/raw/\"\n",
    "\n",
    "X = pd.read_csv(f\"{file_path}x_train_gr_smpl.csv\", delimiter=',')\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = X.loc[[600]].values\n",
    "# image = image[0].reshape((48,48))\n",
    "# image.shape\n",
    "# plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = pd.read_csv(f\"{file_path}y_train_smpl.csv\", delimiter=',')\n",
    "Y.columns = ['target']\n",
    "\n",
    "y0 = pd.read_csv(f\"{file_path}y_train_smpl_0.csv\", delimiter=',')\n",
    "y0.columns = ['target']\n",
    "\n",
    "y1 = pd.read_csv(f\"{file_path}y_train_smpl_1.csv\", delimiter=',')\n",
    "y1.columns = ['target']\n",
    "\n",
    "y2 = pd.read_csv(f\"{file_path}y_train_smpl_2.csv\", delimiter=',')\n",
    "y2.columns = ['target']\n",
    "\n",
    "y3 = pd.read_csv(f\"{file_path}y_train_smpl_3.csv\", delimiter=',')\n",
    "y3.columns = ['target']\n",
    "\n",
    "y4 = pd.read_csv(f\"{file_path}y_train_smpl_4.csv\", delimiter=',')\n",
    "y4.columns = ['target']\n",
    "\n",
    "y5 = pd.read_csv(f\"{file_path}y_train_smpl_5.csv\", delimiter=',')\n",
    "y5.columns = ['target']\n",
    "\n",
    "y6 = pd.read_csv(f\"{file_path}y_train_smpl_6.csv\", delimiter=',')\n",
    "y6.columns = ['target']\n",
    "\n",
    "y7 = pd.read_csv(f\"{file_path}y_train_smpl_7.csv\", delimiter=',')\n",
    "y7.columns = ['target']\n",
    "\n",
    "y8 = pd.read_csv(f\"{file_path}y_train_smpl_8.csv\", delimiter=',')\n",
    "y8.columns = ['target']\n",
    "\n",
    "y9 = pd.read_csv(f\"{file_path}y_train_smpl_9.csv\", delimiter=',')\n",
    "y9.columns = ['target']\n",
    "\n",
    "Y.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_smpl_0 = pd.concat([X, y0], axis=1)\n",
    "train_smpl_1 = pd.concat([X, y1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_smpl_2 = pd.concat([X, y2], axis=1)\n",
    "train_smpl_3 = pd.concat([X, y3], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_smpl_4 = pd.concat([X, y4], axis=1)\n",
    "train_smpl_5 = pd.concat([X, y5], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_smpl_6 = pd.concat([X, y6], axis=1)\n",
    "train_smpl_7 = pd.concat([X, y7], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_smpl_8 = pd.concat([X, y8], axis=1)\n",
    "train_smpl_9 = pd.concat([X, y9], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_smpl = pd.concat([X, Y], axis=1)\n",
    "train_smpl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_smpl_0.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Randomisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn.utils.suffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_smpl = shuffle(train_smpl, random_state=42)\n",
    "train_smpl_0 = shuffle(train_smpl_0, random_state=42)\n",
    "train_smpl_1 = shuffle(train_smpl_1, random_state=42)\n",
    "train_smpl_2 = shuffle(train_smpl_2, random_state=42)\n",
    "train_smpl_3 = shuffle(train_smpl_3, random_state=42)\n",
    "train_smpl_4 = shuffle(train_smpl_4, random_state=42)\n",
    "train_smpl_5 = shuffle(train_smpl_5, random_state=42)\n",
    "train_smpl_6 = shuffle(train_smpl_6, random_state=42)\n",
    "train_smpl_7 = shuffle(train_smpl_7, random_state=42)\n",
    "train_smpl_8 = shuffle(train_smpl_8, random_state=42)\n",
    "train_smpl_9 = shuffle(train_smpl_9, random_state=42)\n",
    "train_smpl.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_smpl.hist(column='target')\n",
    "plt.hist(Y['target'], bins='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_smpl.target.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Reducing the size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Because the data runs the data as a Python file, we do not need to reduce the size of our data set. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_smpl.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler, NearMiss\n",
    "from imblearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ros = RandomOverSampler(random_state=0)\n",
    "# X_oversampled, y_oversampled = ros.fit_resample(X, Y['target'])\n",
    "# plt.hist(y_oversampled, bins='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "oversample_ratio={2: 1000,  6: 1000, 7: 1000, 9: 1000}\n",
    "undersample_ratio={0: 1000, 1: 1000,  3: 1000, 4: 1000, 5: 1000, 8: 1000}\n",
    "pipe = make_pipeline(SMOTE(sampling_strategy=oversample_ratio, n_jobs=7), NearMiss(sampling_strategy=undersample_ratio, n_jobs=7))\n",
    "\n",
    "X_resampled, y_resampled = pipe.fit_resample(X, Y['target'])\n",
    "plt.hist(y_resampled, bins='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# rus = RandomUnderSampler(random_state=0)\n",
    "# X_undersampled, y_undersampled = rus.fit_sample(X, Y)\n",
    "# plt.hist(y_undersampled, bins='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# X_oversampled = pd.DataFrame(data=X_oversampled)\n",
    "# y_oversampled = pd.DataFrame(data=y_oversampled)\n",
    "# y_oversampled.columns=(['target'])\n",
    "# train_oversampled = pd.concat([X_oversampled, y_oversampled], axis=1)\n",
    "# train_oversampled = shuffle(train_oversampled, random_state=42)\n",
    "# train_oversampled.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_resampled = pd.DataFrame(data=X_resampled)\n",
    "y_resampled = pd.DataFrame(data=y_resampled)\n",
    "y_resampled.columns=(['target'])\n",
    "train_resampled = pd.concat([X_resampled, y_resampled], axis=1)\n",
    "train_resampled = shuffle(train_resampled, random_state=42)\n",
    "train_resampled.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# X_undersampled = pd.DataFrame(data=X_undersampled)\n",
    "# y_undersampled = pd.DataFrame(data=y_undersampled)\n",
    "# y_undersampled.columns=(['target'])\n",
    "# train_undersampled = pd.concat([X_undersampled, y_undersampled], axis=1)\n",
    "# train_undersampled = shuffle(train_undersampled, random_state=42)\n",
    "# train_undersampled.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - 6: NB, Features/Attributes Selection, & Improving #5's Classification  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "print(f\"train_smpl shape:{train_smpl.shape}\")\n",
    "\n",
    "best_20 = SelectKBest(chi2, k=20).fit_transform(train_resampled[train_resampled.columns[:2303]], train_resampled['target'])\n",
    "best_20 = pd.DataFrame(best_20)\n",
    "train_smpl_20 = pd.concat([best_20, train_resampled['target']], axis=1)\n",
    "print(f\"train_smpl_20 shape:{train_smpl_20.shape}\")\n",
    "\n",
    "best_50 = SelectKBest(chi2, k=50).fit_transform(train_resampled[train_resampled.columns[:2303]], train_resampled['target'])\n",
    "best_50 = pd.DataFrame(best_50)\n",
    "train_smpl_50 = pd.concat([best_50, train_resampled['target']], axis=1)\n",
    "print(f\"train_smpl_50 shape:{train_smpl_50.shape}\")\n",
    "\n",
    "best_100 = SelectKBest(chi2, k=100).fit_transform(train_resampled[train_resampled.columns[:2303]], train_resampled['target'])\n",
    "best_100 = pd.DataFrame(best_100)\n",
    "train_smpl_100 = pd.concat([best_100, train_resampled['target']], axis=1)\n",
    "print(f\"train_smpl_100 shape:{train_smpl_100.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Train Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_smpl[train_smpl.columns[:2303]], train_smpl['target'], test_size=0.33, random_state=42)\n",
    "# X_train_ovr, X_test_ovr, y_train_ovr, y_test_ovr = train_test_split(train_oversampled[train_oversampled.columns[:2303]], train_oversampled['target'], test_size=0.33, random_state=42)\n",
    "X_train_res, X_test_res, y_train_res, y_test_res = train_test_split(train_resampled[train_resampled.columns[:2303]], train_resampled['target'], test_size=0.33, random_state=42)\n",
    "# X_train_udr, X_test_udr, y_train_udr, y_test_udr = train_test_split(train_undersampled[train_undersampled.columns[:2303]], train_undersampled['target'], test_size=0.33, random_state=42)\n",
    "\n",
    "X_train_20, X_test_20, y_train_20, y_test_20 = train_test_split(train_smpl_20[train_smpl_20.columns[:20]], train_smpl_20['target'], test_size=0.33, random_state=42)\n",
    "X_train_50, X_test_50, y_train_50, y_test_50 = train_test_split(train_smpl_50[train_smpl_50.columns[:50]], train_smpl_50['target'], test_size=0.33, random_state=42)\n",
    "X_train_100, X_test_100, y_train_100, y_test_100 = train_test_split(train_smpl_100[train_smpl_100.columns[:100]], train_smpl_100['target'], test_size=0.33, random_state=42)\n",
    "\n",
    "\n",
    "X_train_0, X_test_0, y_train_0, y_test_0 = train_test_split(train_smpl_0[train_smpl_0.columns[:2303]], train_smpl_0['target'], test_size=0.33, random_state=42)\n",
    "X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(train_smpl_1[train_smpl_1.columns[:2303]], train_smpl_1['target'], test_size=0.33, random_state=42)\n",
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(train_smpl_2[train_smpl_2.columns[:2303]], train_smpl_2['target'], test_size=0.33, random_state=42)\n",
    "X_train_3, X_test_3, y_train_3, y_test_3 = train_test_split(train_smpl_3[train_smpl_3.columns[:2303]], train_smpl_3['target'], test_size=0.33, random_state=42)\n",
    "X_train_4, X_test_4, y_train_4, y_test_4 = train_test_split(train_smpl_4[train_smpl_4.columns[:2303]], train_smpl_4['target'], test_size=0.33, random_state=42)\n",
    "X_train_5, X_test_5, y_train_5, y_test_5 = train_test_split(train_smpl_5[train_smpl_5.columns[:2303]], train_smpl_5['target'], test_size=0.33, random_state=42)\n",
    "X_train_6, X_test_6, y_train_6, y_test_6 = train_test_split(train_smpl_6[train_smpl_6.columns[:2303]], train_smpl_6['target'], test_size=0.33, random_state=42)\n",
    "X_train_7, X_test_7, y_train_7, y_test_7 = train_test_split(train_smpl_7[train_smpl_7.columns[:2303]], train_smpl_7['target'], test_size=0.33, random_state=42)\n",
    "X_train_8, X_test_8, y_train_8, y_test_8 = train_test_split(train_smpl_8[train_smpl_8.columns[:2303]], train_smpl_8['target'], test_size=0.33, random_state=42)\n",
    "X_train_9, X_test_9, y_train_9, y_test_9 = train_test_split(train_smpl_9[train_smpl_9.columns[:2303]], train_smpl_9['target'], test_size=0.33, random_state=42)\n",
    "\n",
    "X_train_10classes = [X_train_0, X_train_1, X_train_2, X_train_3, X_train_4, X_train_5, X_train_6, X_train_7, X_train_8, X_train_9]\n",
    "X_test_10classes = [X_test_0, X_test_1, X_test_2, X_test_3, X_test_4, X_test_5, X_test_6, X_test_7, X_test_8, X_test_9]\n",
    "y_train_10classes = [y_train_0, y_train_1, y_train_2, y_train_3, y_train_4, y_train_5, y_train_6, y_train_7, y_train_8, y_train_9]\n",
    "y_test_10classes = [y_test_0, y_test_1, y_test_2, y_test_3, y_test_4, y_test_5, y_test_6, y_test_7, y_test_8, y_test_9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayes model (multi-class classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before Features/Attributes Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In regards to problem 4: Because we are not using Weka, we did not need to apply any\n",
    "# filters to the data before running Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# clf = MultinomialNB()\n",
    "# clf.fit(X_train_ovr, y_train_ovr)\n",
    "# clf.score(X_test_ovr, y_test_ovr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(X_train_res, y_train_res)\n",
    "clf.score(X_test_res, y_test_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# clf = MultinomialNB()\n",
    "# clf.fit(X_train_udr, y_train_udr)\n",
    "# clf.score(X_test_udr, y_test_udr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred = clf.predict(X_test_res)\n",
    "conf_mat = confusion_matrix(y_test_res, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True):\n",
    "    \"\"\"\n",
    "        https://stackoverflow.com/a/50386871\n",
    "    \"\"\"\n",
    "\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(conf_mat, target_names=y_test.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking for devious class labels that take high numbers of misclassifications\n",
    "for i in range(conf_mat.shape[1]):\n",
    "    column = conf_mat.T[i]\n",
    "    misclassifications = column.sum() - column[i]\n",
    "    print(i,misclassifications)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. For each of the 10 train_smpl_label files, record the first 10 fields, in order of the absolute correlation value for each street sign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "fileList = [train_smpl_0, train_smpl_1,train_smpl_2, train_smpl_3, train_smpl_4,\n",
    "            train_smpl_5, train_smpl_6, train_smpl_7, train_smpl_8, train_smpl_9]\n",
    "for f, file in enumerate(fileList):\n",
    "    corrArr = []\n",
    "    for j in file.columns[:-1]:\n",
    "        corrVal, pVal = stats.pearsonr(file.iloc[:,int(j)], file.iloc[:, -1])\n",
    "        corrArr.append(corrVal**2)\n",
    "    print(f\"File {f}:\")\n",
    "    print(sorted([(x,i) for (i,x) in enumerate(corrArr)], reverse=True)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After Features/Attributes Selection (20/50/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "X_trains_reduced = [  X_train_20, X_train_50, X_train_100]\n",
    "X_tests_reduced = [ X_test_20, X_test_50, X_test_100]\n",
    "y_trains_reduced = [ y_train_20, y_train_50, y_train_100]\n",
    "y_tests_reduced = [ y_test_20, y_test_50, y_test_100]\n",
    "\n",
    "# Gaussian Model on Multiclass dataset ?\n",
    "for X_trains, X_tests, y_trains, y_tests in zip(X_trains_reduced, X_tests_reduced, y_trains_reduced, y_tests_reduced):    \n",
    "    clf = GaussianNB()\n",
    "    clf.fit(X_trains, y_trains)\n",
    "    print(f\"Gaussian NB {X_trains.shape[1]}: {clf.score(X_tests, y_tests)}\")\n",
    "\n",
    "for X_trains, X_tests, y_trains, y_tests in zip(X_trains_reduced, X_tests_reduced, y_trains_reduced, y_tests_reduced):    \n",
    "    clf = MultinomialNB()\n",
    "    clf.fit(X_trains, y_trains)\n",
    "    print(f\"Multinomial NB {X_trains.shape[1]}: {clf.score(X_tests, y_tests)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Naive Bayes model (mono-class classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for X_train, X_test, y_train, y_test in zip(X_train_10classes, X_test_10classes, y_train_10classes, y_test_10classes):    \n",
    "    clf = GaussianNB()\n",
    "    clf.fit(X_train, y_train)\n",
    "    scores.append(clf.score(X_test, y_test))\n",
    "    print(f\"Gaussian NB 10 classes:{clf.score(X_test, y_test)}\")\n",
    "print(\"Gaussian NB Mean: \", sum(scores)/len(scores))\n",
    "\n",
    "scores2 = []\n",
    "for X_train, X_test, y_train, y_test in zip(X_train_10classes, X_test_10classes, y_train_10classes, y_test_10classes):    \n",
    "    clf = MultinomialNB()\n",
    "    clf.fit(X_train, y_train)\n",
    "    scores2.append(clf.score(X_test, y_test))\n",
    "    print(f\"Multinomial NB 10 classes: {clf.score(X_test, y_test)}\")\n",
    "print(\"Multinomial NB Mean: \", sum(scores2)/len(scores2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multinomial Nb is more accurate than Gaussian Nb ( 0.75 > 0.69 ) on binary datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis for 4 & 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Explain the reason for choosing and using these filters. Once you can run the algorithm, record, compare and analyse the classifier’s accuracy on different classes (as given by the Weka Summary and the confusion matrix)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since we are not using Weka, we did not need to apply any filters to the data before running Naive Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. What kind of information about this data set did you learn, as a result of the above experiments? You should ask questions such as: Which streets signs are harder to recognise? Which street signs are most easily confused? Which attributes (fields) are more reliable and which are less reliable in classification of street signs? What was the purpose of Tasks 5 and 6? What would happen if the data sets you used in Tasks 4, 5 and 6 were not randomised? What would happen if there is cross-correlation between the non-class attributes? You will get more marks for more interesting and ``out of the box” questions and answers. Explain your conclusions logically and formally, using the material from the lecture notes and from your own reading to interpret the results that Weka produces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions from initial experiments (q7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the multilabel confusion matrix, it is apparent that the multinomial Naïve Bayes classifier struggled with some classes, predicting them incorrectly most of the time. \n",
    "\n",
    "In particular, class 8 was more often classified as class 1 (probability 0.278), class 7 (probability 0.2023) and class 6 (probability 0.1974) before itself (probability 0.1283).\n",
    "\n",
    "//It looks like the classifier was confused by ...\n",
    "\n",
    "\n",
    "\n",
    "The strongest misclassification in the matrix is true members of class 3 being classified as class 7 (probability 0.2023). However, there is relatively very little of the reverse misclassification (true class 7 predicted as class 3).\n",
    "\n",
    "//^^ investigate this?\n",
    "\n",
    "\n",
    "\n",
    "Class 7 punches above its weight as the most abundant misclassification which is perhaps due to the relative infrequency of data for class 7. There must be some other factor, as there are less populated classes.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
