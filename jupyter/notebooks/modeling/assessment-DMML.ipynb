{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "from scipy import stats\n",
    "import math\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. CSV to ARFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since we are using Python, we do  not need to complete this step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pandas.read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../../data/raw/\"\n",
    "\n",
    "X = pd.read_csv(f\"{file_path}x_train_gr_smpl.csv\", delimiter=',')\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = X.loc[[600]].values\n",
    "# image = image[0].reshape((48,48))\n",
    "# image.shape\n",
    "# plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = pd.read_csv(f\"{file_path}y_train_smpl.csv\", delimiter=',')\n",
    "Y.columns = ['target']\n",
    "\n",
    "y0 = pd.read_csv(f\"{file_path}y_train_smpl_0.csv\", delimiter=',')\n",
    "y0.columns = ['target']\n",
    "\n",
    "y1 = pd.read_csv(f\"{file_path}y_train_smpl_1.csv\", delimiter=',')\n",
    "y1.columns = ['target']\n",
    "\n",
    "y2 = pd.read_csv(f\"{file_path}y_train_smpl_2.csv\", delimiter=',')\n",
    "y2.columns = ['target']\n",
    "\n",
    "y3 = pd.read_csv(f\"{file_path}y_train_smpl_3.csv\", delimiter=',')\n",
    "y3.columns = ['target']\n",
    "\n",
    "y4 = pd.read_csv(f\"{file_path}y_train_smpl_4.csv\", delimiter=',')\n",
    "y4.columns = ['target']\n",
    "\n",
    "y5 = pd.read_csv(f\"{file_path}y_train_smpl_5.csv\", delimiter=',')\n",
    "y5.columns = ['target']\n",
    "\n",
    "y6 = pd.read_csv(f\"{file_path}y_train_smpl_6.csv\", delimiter=',')\n",
    "y6.columns = ['target']\n",
    "\n",
    "y7 = pd.read_csv(f\"{file_path}y_train_smpl_7.csv\", delimiter=',')\n",
    "y7.columns = ['target']\n",
    "\n",
    "y8 = pd.read_csv(f\"{file_path}y_train_smpl_8.csv\", delimiter=',')\n",
    "y8.columns = ['target']\n",
    "\n",
    "y9 = pd.read_csv(f\"{file_path}y_train_smpl_9.csv\", delimiter=',')\n",
    "y9.columns = ['target']\n",
    "\n",
    "Y.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_smpl_0 = pd.concat([X, y0], axis=1)\n",
    "train_smpl_1 = pd.concat([X, y1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_smpl_2 = pd.concat([X, y2], axis=1)\n",
    "train_smpl_3 = pd.concat([X, y3], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_smpl_4 = pd.concat([X, y4], axis=1)\n",
    "train_smpl_5 = pd.concat([X, y5], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_smpl_6 = pd.concat([X, y6], axis=1)\n",
    "train_smpl_7 = pd.concat([X, y7], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_smpl_8 = pd.concat([X, y8], axis=1)\n",
    "train_smpl_9 = pd.concat([X, y9], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_smpl = pd.concat([X, Y], axis=1)\n",
    "train_smpl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_smpl_0.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Randomisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn.utils.suffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_smpl = shuffle(train_smpl, random_state=42)\n",
    "train_smpl_0 = shuffle(train_smpl_0, random_state=42)\n",
    "train_smpl_1 = shuffle(train_smpl_1, random_state=42)\n",
    "train_smpl_2 = shuffle(train_smpl_2, random_state=42)\n",
    "train_smpl_3 = shuffle(train_smpl_3, random_state=42)\n",
    "train_smpl_4 = shuffle(train_smpl_4, random_state=42)\n",
    "train_smpl_5 = shuffle(train_smpl_5, random_state=42)\n",
    "train_smpl_6 = shuffle(train_smpl_6, random_state=42)\n",
    "train_smpl_7 = shuffle(train_smpl_7, random_state=42)\n",
    "train_smpl_8 = shuffle(train_smpl_8, random_state=42)\n",
    "train_smpl_9 = shuffle(train_smpl_9, random_state=42)\n",
    "train_smpl.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_smpl.hist(column='target')\n",
    "plt.hist(Y['target'], bins='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_smpl.target.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Reducing the size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Because the data runs the data as a Python file, we do not need to reduce the size of our data set. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_smpl.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler, NearMiss\n",
    "from imblearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ros = RandomOverSampler(random_state=0)\n",
    "# X_oversampled, y_oversampled = ros.fit_resample(X, Y['target'])\n",
    "# plt.hist(y_oversampled, bins='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "oversample_ratio={2: 1000,  6: 1000, 7: 1000, 9: 1000}\n",
    "undersample_ratio={0: 1000, 1: 1000,  3: 1000, 4: 1000, 5: 1000, 8: 1000}\n",
    "pipe = make_pipeline(SMOTE(sampling_strategy=oversample_ratio, n_jobs=7), NearMiss(sampling_strategy=undersample_ratio, n_jobs=7))\n",
    "\n",
    "X_resampled, y_resampled = pipe.fit_resample(X, Y['target'])\n",
    "plt.hist(y_resampled, bins='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# rus = RandomUnderSampler(random_state=0)\n",
    "# X_undersampled, y_undersampled = rus.fit_sample(X, Y)\n",
    "# plt.hist(y_undersampled, bins='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_oversampled = pd.DataFrame(data=X_oversampled)\n",
    "# y_oversampled = pd.DataFrame(data=y_oversampled)\n",
    "# y_oversampled.columns=(['target'])\n",
    "# train_oversampled = pd.concat([X_oversampled, y_oversampled], axis=1)\n",
    "# train_oversampled = shuffle(train_oversampled, random_state=42)\n",
    "# train_oversampled.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_resampled = pd.DataFrame(data=X_resampled)\n",
    "y_resampled = pd.DataFrame(data=y_resampled)\n",
    "y_resampled.columns=(['target'])\n",
    "train_resampled = pd.concat([X_resampled, y_resampled], axis=1)\n",
    "train_resampled = shuffle(train_resampled, random_state=42)\n",
    "train_resampled.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_undersampled = pd.DataFrame(data=X_undersampled)\n",
    "# y_undersampled = pd.DataFrame(data=y_undersampled)\n",
    "# y_undersampled.columns=(['target'])\n",
    "# train_undersampled = pd.concat([X_undersampled, y_undersampled], axis=1)\n",
    "# train_undersampled = shuffle(train_undersampled, random_state=42)\n",
    "# train_undersampled.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - 6: NB, Features/Attributes Selection, & Improving #5's Classification  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## correlation matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%matplotlib inline\n",
    "\n",
    "#plt.matshow(train_smpl.corr())\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"train_smpl shape:{train_smpl.shape}\")\n",
    "\n",
    "best_20 = SelectKBest(chi2, k=20).fit_transform(train_resampled[train_resampled.columns[:2303]], train_resampled['target'])\n",
    "best_20 = pd.DataFrame(best_20)\n",
    "train_smpl_20 = pd.concat([best_20, train_resampled['target']], axis=1)\n",
    "print(f\"train_smpl_20 shape:{train_smpl_20.shape}\")\n",
    "\n",
    "best_50 = SelectKBest(chi2, k=50).fit_transform(train_resampled[train_resampled.columns[:2303]], train_resampled['target'])\n",
    "best_50 = pd.DataFrame(best_50)\n",
    "train_smpl_50 = pd.concat([best_50, train_resampled['target']], axis=1)\n",
    "print(f\"train_smpl_50 shape:{train_smpl_50.shape}\")\n",
    "\n",
    "best_100 = SelectKBest(chi2, k=100).fit_transform(train_resampled[train_resampled.columns[:2303]], train_resampled['target'])\n",
    "best_100 = pd.DataFrame(best_100)\n",
    "train_smpl_100 = pd.concat([best_100, train_resampled['target']], axis=1)\n",
    "print(f\"train_smpl_100 shape:{train_smpl_100.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Train Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_smpl[train_smpl.columns[:2303]], train_smpl['target'], test_size=0.33, random_state=42)\n",
    "# X_train_ovr, X_test_ovr, y_train_ovr, y_test_ovr = train_test_split(train_oversampled[train_oversampled.columns[:2303]], train_oversampled['target'], test_size=0.33, random_state=42)\n",
    "X_train_res, X_test_res, y_train_res, y_test_res = train_test_split(train_resampled[train_resampled.columns[:2303]], train_resampled['target'], test_size=0.33, random_state=42)\n",
    "# X_train_udr, X_test_udr, y_train_udr, y_test_udr = train_test_split(train_undersampled[train_undersampled.columns[:2303]], train_undersampled['target'], test_size=0.33, random_state=42)\n",
    "\n",
    "X_train_20, X_test_20, y_train_20, y_test_20 = train_test_split(train_smpl_20[train_smpl_20.columns[:20]], train_smpl_20['target'], test_size=0.33, random_state=42)\n",
    "X_train_50, X_test_50, y_train_50, y_test_50 = train_test_split(train_smpl_50[train_smpl_50.columns[:50]], train_smpl_50['target'], test_size=0.33, random_state=42)\n",
    "X_train_100, X_test_100, y_train_100, y_test_100 = train_test_split(train_smpl_100[train_smpl_100.columns[:100]], train_smpl_100['target'], test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayes model (multi-class classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before Features/Attributes Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In regards to problem 4: Because we are not using Weka, we did not need to apply any\n",
    "# filters to the data before running Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# clf = MultinomialNB()\n",
    "# clf.fit(X_train_ovr, y_train_ovr)\n",
    "# clf.score(X_test_ovr, y_test_ovr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(X_train_res, y_train_res)\n",
    "clf.score(X_test_res, y_test_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# clf = MultinomialNB()\n",
    "# clf.fit(X_train_udr, y_train_udr)\n",
    "# clf.score(X_test_udr, y_test_udr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test_res)\n",
    "conf_mat = confusion_matrix(y_test_res, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True):\n",
    "    \"\"\"\n",
    "        https://stackoverflow.com/a/50386871\n",
    "    \"\"\"\n",
    "\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(conf_mat, target_names=y_test.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking for devious class labels that take high numbers of misclassifications\n",
    "for i in range(conf_mat.shape[1]):\n",
    "    column = conf_mat.T[i]\n",
    "    misclassifications = column.sum() - column[i]\n",
    "    print(i,misclassifications)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Naive Bayes on Multilabel dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n",
    "print(f\"Multinomial NB: {clf.score(X_test, y_test)} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. For each of the 10 train_smpl_label files, record the first 10 fields, in order of the absolute correlation value for each street sign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileList = [train_smpl_0, train_smpl_1, train_smpl_2, train_smpl_3, train_smpl_4,\n",
    "            train_smpl_5, train_smpl_6, train_smpl_7, train_smpl_8, train_smpl_9]\n",
    "for f, file in enumerate(fileList):\n",
    "    corrArr = []\n",
    "    for j in file.columns[:-1]:\n",
    "        corrVal, pVal = stats.pearsonr(file.iloc[:, int(j)], file.iloc[:, -1])\n",
    "        # Record the absolute value of the correlation\n",
    "        corrArr.append(math.fabs(corrVal))\n",
    "    print(f\"File {f}:\")\n",
    "    print(sorted([(x, i) for (i, x) in enumerate(corrArr)], reverse=True)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Naive Bayes using select features (20, 50, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods to run NB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_multi_nb_binary(data, num_feat, class_num):\n",
    "    X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(data[data.columns[:2303]], data['target'],\n",
    "                                                                test_size=0.33, random_state=42)\n",
    "    clf = MultinomialNB()\n",
    "    clf.fit(X_train_2, y_train_2)\n",
    "    print(f\"Multinomial for Binary NB Best {num_feat}0: Class {class_num}: {clf.score(X_test_2, y_test_2)}\")\n",
    "\n",
    "def run_multi_nb(data, num_feat):\n",
    "    X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(data[data.columns[:2303]], data['target'],\n",
    "                                                                test_size=0.33, random_state=42)\n",
    "    clf = MultinomialNB()\n",
    "    clf.fit(X_train_2, y_train_2)\n",
    "    print(f\"Multinomial NB Best {num_feat}0: {clf.score(X_test_2, y_test_2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Select top 2, 5, and 10 features for all train_smpl_label files to create best 20, 50, and 100 sets for all 10 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestSet = [2, 5, 10]\n",
    "\n",
    "for i, num in enumerate(bestSet):\n",
    "    k_best_list = pd.DataFrame()\n",
    "    # Get top k for each 10 classes\n",
    "    for f, file in enumerate(fileList):\n",
    "        k_best = SelectKBest(chi2, k=num).fit_transform(file[file.columns[:2303]], file['target'])\n",
    "        k_best = pd.DataFrame(k_best)\n",
    "        k_best_list = pd.concat([k_best_list, k_best], axis=1)\n",
    "        # print(k_best_list.head())\n",
    "\n",
    "    print(f\"\\nBinary NB Classifier {num}0:\\n __________________________________\")\n",
    "    # run binary NB classifiers for each class\n",
    "    for f, file in enumerate(fileList):\n",
    "        k_best_binary = pd.concat([k_best_list, file['target'].reset_index()], axis=1)\n",
    "        # print(f\"K BEST BINARY: {k_best_binary}\")\n",
    "        run_multi_nb_binary(k_best_binary, num, f)\n",
    "\n",
    "    print(f\"\\nMultilabel NB Classifier {num}0:\\n __________________________________\")\n",
    "    k_best_multi = pd.concat([k_best_list, train_smpl['target'].reset_index()], axis=1)\n",
    "    # print(f\"K BEST MULTI: {k_best_multi}\")\n",
    "    run_multi_nb(k_best_multi, num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis for 4 & 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Explain the reason for choosing and using these filters. Once you can run the algorithm, record, compare and analyse the classifier’s accuracy on different classes (as given by the Weka Summary and the confusion matrix)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since we are not using Weka, we did not need to apply any filters to the data before running Naive Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. What kind of information about this data set did you learn, as a result of the above experiments? You should ask questions such as: Which streets signs are harder to recognise? Which street signs are most easily confused? Which attributes (fields) are more reliable and which are less reliable in classification of street signs? What was the purpose of Tasks 5 and 6? What would happen if the data sets you used in Tasks 4, 5 and 6 were not randomised? What would happen if there is cross-correlation between the non-class attributes? You will get more marks for more interesting and ``out of the box” questions and answers. Explain your conclusions logically and formally, using the material from the lecture notes and from your own reading to interpret the results that Weka produces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions from initial experiments (q7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the multilabel confusion matrix, it is apparent that the multinomial Naïve Bayes classifier struggled with some classes, predicting them incorrectly most of the time. \n",
    "\n",
    "In particular, class 8 was more often classified as class 1 (probability 0.278), class 7 (probability 0.2023) and class 6 (probability 0.1974) before itself (probability 0.1283).\n",
    "\n",
    "//It looks like the classifier was confused by ...\n",
    "\n",
    "\n",
    "\n",
    "The strongest misclassification in the matrix is true members of class 3 being classified as class 7 (probability 0.2023). However, there is relatively very little of the reverse misclassification (true class 7 predicted as class 3).\n",
    "\n",
    "//^^ investigate this?\n",
    "\n",
    "\n",
    "\n",
    "Class 7 punches above its weight as the most abundant misclassification which is perhaps due to the relative infrequency of data for class 7. There must be some other factor, as there are less populated classes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complex Bayesian Networks\n",
    "\n",
    "scikit learn has naive bayes builtin, but not the more complex networks. Weka can be called on to fill the gap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K2 Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weka.core.jvm as jvm\n",
    "\n",
    "jvm.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I added the str call to this line (HM):\n",
    "# arffList.append(\"@attribute \" + str(df.columns[i]) + \" real\\n\") \n",
    "\n",
    "# ---\n",
    "\n",
    "# https://github.com/saurabhnagrecha/Pandas-to-ARFF\n",
    "\n",
    "# MIT License\n",
    "\n",
    "# Copyright (c) [2015] [Saurabh Nagrecha]\n",
    "\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def pandas2arff(df,filename,wekaname = \"pandasdata\",cleanstringdata=True,cleannan=True):\n",
    "    \"\"\"\n",
    "    converts the pandas dataframe to a weka compatible file\n",
    "    df: dataframe in pandas format\n",
    "    filename: the filename you want the weka compatible file to be in\n",
    "    wekaname: the name you want to give to the weka dataset (this will be visible to you when you open it in Weka)\n",
    "    cleanstringdata: clean up data which may have spaces and replace with \"_\", special characters etc which seem to annoy Weka. \n",
    "                     To suppress this, set this to False\n",
    "    cleannan: replaces all nan values with \"?\" which is Weka's standard for missing values. \n",
    "              To suppress this, set this to False\n",
    "    \"\"\"\n",
    "    import re\n",
    "    \n",
    "    def cleanstring(s):\n",
    "        if s!=\"?\":\n",
    "            return re.sub('[^A-Za-z0-9]+', \"_\", str(s))\n",
    "        else:\n",
    "            return \"?\"\n",
    "            \n",
    "    dfcopy = df #all cleaning operations get done on this copy\n",
    "\n",
    "    \n",
    "    if cleannan!=False:\n",
    "        dfcopy = dfcopy.fillna(-999999999) #this is so that we can swap this out for \"?\"\n",
    "        #this makes sure that certain numerical columns with missing values don't get stuck with \"object\" type\n",
    " \n",
    "    f = open(filename,\"w\")\n",
    "    arffList = []\n",
    "    arffList.append(\"@relation \" + wekaname + \"\\n\")\n",
    "    #look at each column's dtype. If it's an \"object\", make it \"nominal\" under Weka for now (can be changed in source for dates.. etc)\n",
    "    for i in range(df.shape[1]):\n",
    "        if i>2302:\n",
    "            print(i)\n",
    "#             print(dfcopy.dtypes[i])\n",
    "            print(df.columns[i])\n",
    "        \n",
    "        # if dfcopy.dtypes[i]=='O' or\n",
    "        if (df.columns[i] in [\"Class\",\"CLASS\",\"class\",'target']):\n",
    "            if cleannan!=False:\n",
    "                dfcopy.iloc[:,i] = dfcopy.iloc[:,i].replace(to_replace=-999999999, value=\"?\")\n",
    "            if cleanstringdata!=False:\n",
    "                dfcopy.iloc[:,i] = dfcopy.iloc[:,i].apply(cleanstring)\n",
    "            _uniqueNominalVals = [str(_i) for _i in np.unique(dfcopy.iloc[:,i])]\n",
    "            _uniqueNominalVals = \",\".join(_uniqueNominalVals)\n",
    "            _uniqueNominalVals = _uniqueNominalVals.replace(\"[\",\"\")\n",
    "            _uniqueNominalVals = _uniqueNominalVals.replace(\"]\",\"\")\n",
    "            _uniqueValuesString = \"{\" + _uniqueNominalVals +\"}\" \n",
    "            arffList.append(\"@attribute \" + df.columns[i] + _uniqueValuesString + \"\\n\")\n",
    "        else:\n",
    "            arffList.append(\"@attribute \" + str(df.columns[i]) + \" real\\n\") \n",
    "            #even if it is an integer, let's just deal with it as a real number for now\n",
    "    arffList.append(\"@data\\n\")           \n",
    "    for i in range(dfcopy.shape[0]):#instances\n",
    "        _instanceString = \"\"\n",
    "        for j in range(df.shape[1]):#features\n",
    "#                 if dfcopy.dtypes[j]=='O':\n",
    "#                     _instanceString+=\"\\\"\" + str(dfcopy.iloc[i,j]) + \"\\\"\"\n",
    "#                 else:\n",
    "                _instanceString+=str(dfcopy.iloc[i,j])\n",
    "                if j!=dfcopy.shape[1]-1:#if it's not the last feature, add a comma\n",
    "                    _instanceString+=\",\"\n",
    "        _instanceString+=\"\\n\"\n",
    "        if cleannan!=False:\n",
    "            _instanceString = _instanceString.replace(\"-999999999.0\",\"?\") #for numeric missing values\n",
    "            _instanceString = _instanceString.replace(\"\\\"?\\\"\",\"?\") #for categorical missing values\n",
    "        arffList.append(_instanceString)\n",
    "    print(\"Writing lines...\")\n",
    "    f.writelines(arffList)\n",
    "    print(\"Closing the file...\")\n",
    "    f.close()\n",
    "    print(\"Deleting the copy...\")\n",
    "    del dfcopy\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_20 = pd.concat([X_train_50, y_train_50], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output to arff for verification in Weka gui\n",
    "pandas2arff(data_train_20, 'data_train_20.arff')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weka.core.classes as wkcore\n",
    "from weka.classifiers import Classifier, Evaluation\n",
    "import os\n",
    "\n",
    "# cmdline doesn't seem to be working - use the basic classes directly (or at least using the options list)\n",
    "# cls = Classifier(classname=\"weka.classifiers.trees.J48\", options=[\"-C\", \"0.3\"])\n",
    "\n",
    "# k2, 1-4 parents:  \n",
    "# cmdline = 'weka.classifiers.bayes.BayesNet -D -Q weka.classifiers.bayes.net.search.local.K2 -- -P 1 -S BAYES -E weka.classifiers.bayes.net.estimate.SimpleEstimator -- -A 0.5'\n",
    "# cmdline = 'weka.classifiers.bayes.BayesNet -D -Q weka.classifiers.bayes.net.search.local.K2 -- -P 2 -S BAYES -E weka.classifiers.bayes.net.estimate.SimpleEstimator -- -A 0.5'\n",
    "# cmdline = 'weka.classifiers.bayes.BayesNet -D -Q weka.classifiers.bayes.net.search.local.K2 -- -P 3 -S BAYES -E weka.classifiers.bayes.net.estimate.SimpleEstimator -- -A 0.5'\n",
    "# cmdline = 'weka.classifiers.bayes.BayesNet -D -Q weka.classifiers.bayes.net.search.local.K2 -- -P 4 -S BAYES -E weka.classifiers.bayes.net.estimate.SimpleEstimator -- -A 0.5'\n",
    "\n",
    "# TAN \n",
    "# cmdline = 'weka.classifiers.bayes.BayesNet -D -Q weka.classifiers.bayes.net.search.local.TAN -- -S BAYES -E weka.classifiers.bayes.net.estimate.SimpleEstimator -- -A 0.5'\n",
    "# classifier = wkcore.from_commandline(cmdline, classname=\"weka.classifiers.Classifier\")\n",
    "# classifier = Classifier(classname=\"weka.classifiers.trees.J48\", options=[\"-C\", \"0.3\"])\n",
    "classifier = Classifier(classname=\"weka.classifiers.bayes.BayesNet\", options=[\"-D\", \"-Q\", \"weka.classifiers.bayes.net.search.local.TAN\", \"--\", \"-S\", \"BAYES\", \"-E\", \"weka.classifiers.bayes.net.estimate.SimpleEstimator\", \"--\", \"-A\", \"0.5\"])\n",
    "\n",
    "\n",
    "# classifier.build_classifier(iris_data)\n",
    "\n",
    "# weka.classifiers.bayes.BayesNet -D -Q weka.classifiers.bayes.net.search.local.TAN -- -S BAYES -E weka.classifiers.bayes.net.estimate.SimpleEstimator -- -A 0.5\n",
    "print(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "# Don't downsize - use it all!\n",
    "# X_train_wee, _, y_train_wee, _ = train_test_split(X_train_res, y_train_res, test_size=0.95) \n",
    "X_train_2D_t_emb = TSNE(perplexity=45).fit_transform(X_train_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# receive arff\n",
    "# from weka.core.converters import Loader\n",
    "# loader = Loader(classname=\"weka.core.converters.ArffLoader\")\n",
    "# arffData = loader.load_file(os.getcwd() + '/' + 'data_train_20.arff')\n",
    "\n",
    "# Alternatively, ndarray_to_instances\n",
    "import weka.core.converters as wekaconv\n",
    "print(X_train_2D_t_emb.shape)\n",
    "y_train_res_np = y_train_res.to_numpy().reshape(-1, 1)\n",
    "print(y_train_wee_np.shape)\n",
    "\n",
    "arffData = wekaconv.ndarray_to_instances(np.append(X_train_2D_t_emb, values=y_train_res_np, axis=1), \"tsne_subset\")\n",
    "arffData.class_is_last()\n",
    "\n",
    "# clean up\n",
    "# os.remove('pandasdata.arff')\n",
    "\n",
    "\n",
    "# print(arffData.class_attribute)\n",
    "print(arffData.get_instance(0))\n",
    "print(arffData.get_instance(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix \"class is numeric\"\n",
    "# First create a Filter to do the job\n",
    "from weka.filters import Filter\n",
    "nominaliser = Filter(classname='weka.filters.unsupervised.attribute.NumericToNominal', options=['-R', 'last']) \n",
    "nominaliser.inputformat(arffData)\n",
    "\n",
    "# # Second, apply the filter to the classifer\n",
    "# # filteredClassifier doesn't seem to be working - use the simple classes in series.\n",
    "# from weka.classifiers import FilteredClassifier\n",
    "# filteredClassifier = FilteredClassifier()\n",
    "# filteredClassifier.filter = nominaliser\n",
    "# filteredClassifier.classifier = classifier\n",
    "\n",
    "# filtered...\n",
    "filteredData = nominaliser.filter(arffData)\n",
    "from weka.core.converters import Saver\n",
    "saver = Saver(classname=\"weka.core.converters.ArffSaver\")\n",
    "saver.save_file(filteredData, \"filtered.arff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.build_classifier(filteredData)\n",
    "print(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weka.core.classes import Random\n",
    "\n",
    "evaluation = Evaluation(filteredData)\n",
    "# print (evaluation.evaluate_model())\n",
    "evaluation.evaluate_train_test_split(classifier, filteredData, 66.7, rnd=Random(42))\n",
    "print(evaluation.header)\n",
    "\n",
    "\n",
    "print(f\"Instances: {filteredData.num_instances}\\n\")\n",
    "print(f\"Attributes: {filteredData.num_attributes}\\n\")\n",
    "print(f\"{evaluation.percent_correct}% correct\")\n",
    "print(f\"{evaluation.percent_incorrect}% incorrect\")\n",
    "\n",
    "\n",
    "print(f\"{evaluation.precision(filteredData.class_index)} precision\")\n",
    "print(f\"{evaluation.recall(filteredData.class_index)} recall\")\n",
    "\n",
    "print (evaluation.confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import weka.plot.graph as wekagraph  # pygraphviz required here\n",
    "# # wekagraph.plot_dot_graph(classifier.graph)\n",
    "# # print(classifier.graph)\n",
    "\n",
    "# # import pydot as pydot\n",
    "\n",
    "# # pydot.graph_from_dot_data(classifier.graph)\n",
    "\n",
    "# import os\n",
    "# print(os.environ['PATH'])\n",
    "\n",
    "\n",
    "# ## https://groups.google.com/forum/#!topic/python-weka-wrapper/NxpViz49554\n",
    "# # https://docs.google.com/viewer?a=v&pid=forums&srcid=MTM1ODQ2MTMwNjI4MjExNjM3MjcBMDc1Njk1MTUxNTYzNjczMDA1NTYBZEZDM1IyYXlCd0FKATAuMQEBdjI&authuser=0\n",
    "# import subprocess\n",
    "# import traceback\n",
    "# from PIL import Image\n",
    "# # not available on Windows 64bit:\n",
    "# #plot_graph.plot_dot_graph(classifier.graph)\n",
    "# # call dot manually\n",
    "# graph_file = \"graph.dot\"\n",
    "# png_file = \"graph.png\"\n",
    "# print(\"\\n--> writing graph file:\\n\")\n",
    "# with open(graph_file, \"w\") as text_file:\n",
    "#     text_file.write(classifier.graph)\n",
    "# params = [\"dot\", \"-Tpng\", \"-o\" + png_file, graph_file]\n",
    "# print(\"\\n--> calling dot \\n\")\n",
    "# print(params)\n",
    "# try:\n",
    "#     completed = subprocess.call(params)\n",
    "#     if completed.returncode == 0:\n",
    "#         print(\"--> successful!\")\n",
    "#         image = Image.open(png_file)\n",
    "#         image.show()\n",
    "#     else:\n",
    "#         print(\"--> exit code:\", completed.returncode)\n",
    "#         print(\"--> stdout\")\n",
    "#         print(completed.stdout)\n",
    "#         print(\"--> stderr\")\n",
    "#         print(completed.returncode)\n",
    "# except:\n",
    "#     print(traceback.format_exc())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# scaler = MinMaxScaler(feature_range=[0, 1])\n",
    "\n",
    "# pca = PCA(n_components=50)\n",
    "# smpl_data = pd.DataFrame(pca.fit_transform(train_resampled[train_resampled.columns[:-1]]))\n",
    "# smpl_data = pd.DataFrame(scaler.fit_transform(smpl_data))\n",
    "# smpl_data[\"target\"] = train_smpl[\"target\"]\n",
    "\n",
    "# Training sample for train_smpl\n",
    "\n",
    "smpl_data = train_smpl.copy()\n",
    "\n",
    "kmeans = KMeans(n_clusters=10, random_state=2)\n",
    "smpl_data[\"cluster\"] = kmeans.fit_predict(smpl_data[smpl_data.columns[:-1]])\n",
    "# smpl_data[\"w_class_attribute\"] = kmeans.fit_predict(smpl_data[smpl_data.columns[:-1]])\n",
    "\n",
    "resampled_data = train_resampled.copy()\n",
    "\n",
    "kmeans = KMeans(n_clusters=10, random_state=2)\n",
    "resampled_data[\"cluster\"] = kmeans.fit_predict(resampled_data[resampled_data.columns[:-1]])\n",
    "# resampled_data[\"w_class_attribute\"] = kmeans.fit_predict(resampled_data[resampled_data.columns[:-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Training sample for train_smpl_<label> \n",
    "training_dataframe = [train_smpl_0.copy(), train_smpl_1.copy(), train_smpl_2.copy(), train_smpl_3.copy(), train_smpl_4.copy(),\n",
    "                      train_smpl_5.copy(), train_smpl_6.copy(), train_smpl_7.copy(), train_smpl_8.copy(), train_smpl_9.copy()]\n",
    "\n",
    "kmeans = KMeans(n_clusters=2, random_state=2)\n",
    "for dataframe in training_dataframe:\n",
    "    dataframe[\"cluster\"] = kmeans.fit_predict(dataframe[dataframe.columns[:-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "#Fitting the PCA algorithm with our Data\n",
    "# pca = PCA().fit(data_rescaled)\n",
    "\n",
    "#Plotting the Cumulative Summation of the Explained Variance\n",
    "# plt.figure()\n",
    "# plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "# plt.xlabel('Number of Components')\n",
    "# plt.ylabel('Variance (%)') #for each component\n",
    "# plt.title('Pulsar Dataset Explained Variance')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# print(target.groupby(['target']).count())\n",
    "# result_class_attribute = result.groupby(['target', 'cluster_class_attribute']).count()\n",
    "# result_cluster[\"class\"] = result_class_attribute[\"cluster\"]\n",
    "\n",
    "def plot_clustering(result, nb_attribute, nb_cluster, name_cluster=\"cluster\"):\n",
    "    df = result[result.columns[-3:]]\n",
    "    result_cluster = df.groupby(['target', name_cluster]).count()\n",
    "    result_cluster.reset_index(inplace=True)\n",
    "    result_data = []\n",
    "    for i in range(0, nb_cluster):\n",
    "        result_data.append([])\n",
    "        if name_cluster == \"w_class_attribute\":\n",
    "            data = result_cluster[result_cluster.w_class_attribute == i].drop([name_cluster], axis=1).to_dict('split')['data']\n",
    "        else:\n",
    "            data = result_cluster[result_cluster.cluster == i].drop([name_cluster], axis=1).to_dict('split')['data']\n",
    "        data = [{x[0]: x[1]} for x in data]\n",
    "        data = dict(pair for d in data for pair in d.items())\n",
    "        for j in range(0, nb_attribute):\n",
    "            result_data[i].append(data[j] if j in data else 0)\n",
    "\n",
    "    data = np.array(result_data)\n",
    "\n",
    "    color_list = ['#FC7A57', '#EEFC57', '#5E5B52', '#8DB580', '#BBE5ED', '#4281A4', '#BAD9B5', '#CA7DF9', '#04E762', '#723D46']\n",
    "\n",
    "    X = np.arange(data.shape[1])\n",
    "    for i in range(data.shape[0]):\n",
    "        plt.bar(X, data[i],\n",
    "            bottom = np.sum(data[:i], axis = 0),\n",
    "            color = color_list[i % len(color_list)],\n",
    "               label=f\"Cluster {i}\")\n",
    "\n",
    "    plt.legend(loc=\"best\", bbox_to_anchor=(1.0, 1.00))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Plot train_smpl_<label> clusters\n",
    "i = 0\n",
    "for dataframe in training_dataframe:\n",
    "    print(f\"Training sample n°{i}:\")\n",
    "    plot_clustering(dataframe, 10, 2)\n",
    "    i += 1\n",
    "\"\"\"\n",
    "# Plot train_smpl clusters\n",
    "print(f\"Training sample all data (without class attribute):\")\n",
    "plot_clustering(smpl_data, 10, 10)\n",
    "# print(f\"Training sample all data (with class attribute):\")\n",
    "# plot_clustering(smpl_data, 10, 10, 'w_class_attribute')\n",
    "\n",
    "# Plot train_smpl resampled clusters\n",
    "print(f\"Training resample data (without class attribute):\")\n",
    "plot_clustering(resampled_data, 10, 10)\n",
    "# print(f\"Training resample data (with class attribute):\")\n",
    "# plot_clustering(resampled_data, 10, 10, 'w_class_attribute')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Clustering notes\n",
    "\n",
    "The clustering classification is not as good as the Gaussian Naives model. It struggle with attributing the classes to the good target.\n",
    "\n",
    "When it is trained with the individual sample the two cluster doesn't separate the specific class from the other classes. But for the sample n°2 all the class is in one cluster, but the cluster also contains a lot of other classes.\n",
    "\n",
    "The classification obtained with and without class attribute are the same. Maybe we should change weight on the class attribute so there is more impact on the distribution.\n",
    "\n",
    "I did some tests with using PCA (Principal component analysis) which convert a set of observations of possibly correlated variables to reduce the number of attributes of the samples. And some other with the MinMax scaler. But theses preprocess didn't improve the result, the cluster are still very distributed among the targets.\n",
    "\n",
    "When we are comparing the result between basic sample of all data and the resample one, neither of which seems to be better than the other.\n",
    "\n",
    "But we can notice in the resample result, that for the target 6 and 7, 9 they have almost a dedicated a cluster (corresponding in the order of cluster 7, 9 and 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Other clustering algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "smpl_data = train_resampled.copy()\n",
    "\n",
    "dbscan = DBSCAN(eps=100, min_samples=10)\n",
    "smpl_data[\"cluster\"] = dbscan.fit_predict(smpl_data[smpl_data.columns[:-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import Birch\n",
    "\n",
    "smpl_data = train_resampled.copy()\n",
    "\n",
    "birch = Birch(threshold=0.5, branching_factor=50, n_clusters=8)\n",
    "smpl_data[\"cluster\"] = birch.fit_predict(smpl_data[smpl_data.columns[:-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MeanShift\n",
    "\n",
    "print('a')\n",
    "smpl_data = train_resampled.copy()\n",
    "\n",
    "mean_shift = MeanShift(bandwidth=100)\n",
    "smpl_data[\"cluster\"] = mean_shift.fit_predict(smpl_data[smpl_data.columns[:-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(dbscan.labels_)\n",
    "\n",
    "plot_clustering(smpl_data, 10, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
